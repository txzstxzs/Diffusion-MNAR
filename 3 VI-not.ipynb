{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7cca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from engine.logger import Logger\n",
    "from engine.solver import Trainer\n",
    "from Data.build_dataloader import build_dataloader, build_dataloader_cond\n",
    "from Models.interpretable_diffusion.model_utils import unnormalize_to_zero_to_one\n",
    "from Utils.io_utils import load_yaml_config, seed_everything, merge_opts_to_config, instantiate_from_config\n",
    "from Utils.metric_utils import visualization\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from ema_pytorch import EMA\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from Utils.io_utils import instantiate_from_config, get_model_parameters_info\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font',family='Times New Roman') \n",
    "\n",
    "import umap                         \n",
    "import seaborn as sns \n",
    "from tqdm import tqdm\n",
    "from einops import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a88cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Training Script')\n",
    "    parser.add_argument('--name', type=str, default=None)\n",
    "\n",
    "    parser.add_argument('--config_file', type=str, default=None, \n",
    "                        help='path of config file')\n",
    "    parser.add_argument('--output', type=str, default='OUTPUT', \n",
    "                        help='directory to save the results')\n",
    "    parser.add_argument('--tensorboard', action='store_true', \n",
    "                        help='use tensorboard for logging')\n",
    "\n",
    "    # args for random\n",
    "\n",
    "    parser.add_argument('--cudnn_deterministic', action='store_true', default=False,\n",
    "                        help='set cudnn.deterministic True')\n",
    "    parser.add_argument('--seed', type=int, default=10, \n",
    "                        help='seed for initializing training.')\n",
    "    parser.add_argument('--gpu', type=int, default=None,\n",
    "                        help='GPU id to use. If given, only the specific gpu will be'\n",
    "                        ' used, and ddp will be disabled')\n",
    "    \n",
    "    # args for training\n",
    "    parser.add_argument('--train', action='store_true', default=False, help='Train or Test.')\n",
    "    parser.add_argument('--sample', type=int, default=0, \n",
    "                        choices=[0, 1], help='Condition or Uncondition.')\n",
    "    parser.add_argument('--mode', type=str, default='infill',\n",
    "                        help='Infilling or Forecasting.')\n",
    "    parser.add_argument('--milestone', type=int, default=10)\n",
    "\n",
    "    parser.add_argument('--missing_ratio', type=float, default=0., help='Ratio of Missing Values.')\n",
    "    parser.add_argument('--pred_len', type=int, default=0, help='Length of Predictions.')\n",
    "    \n",
    "    \n",
    "    # args for modify config\n",
    "    parser.add_argument('opts', help='Modify config options using the command-line',\n",
    "                        default=None, nargs=argparse.REMAINDER)  \n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.save_dir = os.path.join(args.output, f'{args.name}')\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a503d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(name=None, config_file='./Config/SP500.yaml', output='OUTPUT', tensorboard=False, cudnn_deterministic=False, seed=10, gpu=0, train=True, sample=0, mode='infill', milestone=10, missing_ratio=0.0, pred_len=0, opts=[], save_dir='OUTPUT\\\\None')\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "\n",
    "data_name = 'SP500'\n",
    "# data_name = 'ETTh1'\n",
    "# data_name = 'ER'\n",
    "# data_name = 'Energy'\n",
    "# data_name = 'weather'\n",
    "\n",
    "if data_name == 'SP500':\n",
    "    args.config_file =  './Config/SP500.yaml'\n",
    "\n",
    "elif data_name == 'ER':\n",
    "    args.config_file =  './Config/exchange_rate.yaml'\n",
    "    \n",
    "elif data_name == 'ETTh1':\n",
    "    args.config_file =  './Config/ETTh1.yaml'\n",
    "\n",
    "elif data_name == 'Energy':\n",
    "    args.config_file =  './Config/Energy.yaml'\n",
    "    \n",
    "elif data_name == 'weather':\n",
    "    args.config_file =  './Config/weather.yaml'\n",
    "    \n",
    "elif data_name == 'MuJoCo':\n",
    "    args.config_file =  './Config/MuJoCo.yaml'\n",
    "    \n",
    "\n",
    "args.gpu =  0\n",
    "args.train = True\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3918163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 10\n",
      "{'model': {'target': 'Models.interpretable_diffusion.gaussian_diffusion.Diffusion_TS', 'params': {'seq_length': 30, 'feature_size': 6, 'n_layer_enc': 2, 'n_layer_dec': 2, 'd_model': 64, 'timesteps': 500, 'sampling_timesteps': 500, 'loss_type': 'l1', 'beta_schedule': 'cosine', 'n_heads': 4, 'mlp_hidden_times': 4, 'attn_pd': 0.0, 'resid_pd': 0.0, 'kernel_size': 1, 'padding_size': 0}}, 'solver': {'base_lr': 1e-05, 'max_epochs': 10000, 'results_folder': './Checkpoints_stock', 'gradient_accumulate_every': 2, 'save_cycle': 500, 'ema': {'decay': 0.995, 'update_interval': 10}, 'scheduler': {'target': 'engine.lr_sch.ReduceLROnPlateauWithWarmup', 'params': {'factor': 0.5, 'patience': 2000, 'min_lr': 1e-05, 'threshold': 0.1, 'threshold_mode': 'rel', 'warmup_lr': 0.0008, 'warmup': 500, 'verbose': False}}}, 'dataloader': {'train_dataset': {'target': 'Utils.Data_utils.real_datasets.CustomDataset', 'params': {'name': 'stock', 'proportion': 1.0, 'data_root': './Data/datasets/stock_data.csv', 'window': 24, 'save2npy': True, 'neg_one_to_one': True, 'seed': 123, 'period': 'train'}}, 'test_dataset': {'target': 'Utils.Data_utils.real_datasets.CustomDataset', 'params': {'name': 'stock', 'proportion': 0.9, 'data_root': './Data/datasets/stock_data.csv', 'window': 24, 'save2npy': True, 'neg_one_to_one': True, 'seed': 123, 'period': 'test', 'style': 'separate', 'distribution': 'geometric'}, 'coefficient': 0.01, 'step_size': 0.05, 'sampling_steps': 200}, 'batch_size': 128, 'sample_size': 256, 'shuffle': True}}\n",
      "cosine 500\n"
     ]
    }
   ],
   "source": [
    "if args.seed is not None:\n",
    "    seed_everything(args.seed)\n",
    "\n",
    "if args.gpu is not None:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "config = load_yaml_config(args.config_file)\n",
    "config = merge_opts_to_config(config, args.opts)\n",
    "\n",
    "\n",
    "config['dataloader']['batch_size'] = 128\n",
    "print(config)\n",
    "\n",
    "beta_schedule = config['model']['params']['beta_schedule'] \n",
    "timesteps = config['model']['params']['timesteps'] \n",
    "print(beta_schedule, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f1c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(args)\n",
    "logger.save_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4efbc40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 500 l1\n",
      "500 False\n"
     ]
    }
   ],
   "source": [
    "model = instantiate_from_config(config['model']).cuda()\n",
    "print(model.use_ff, model.num_timesteps, model.loss_type)\n",
    "print(model.sampling_timesteps, model.fast_sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39db9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3406e18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if data_name == 'SP500':\n",
    "    load_data = np.load( './Data/datasets/sp500.npy')\n",
    "\n",
    "elif data_name == 'ER':\n",
    "    load_data = np.load( './Data/datasets/exchange_rate.npy')\n",
    "\n",
    "elif data_name == 'ETTh1':\n",
    "    load_data = np.load( './Data/datasets/ETTh1.npy')\n",
    "    \n",
    "elif data_name == 'Energy':\n",
    "    load_data = np.load( './Data/datasets/Energy.npy')\n",
    "    \n",
    "elif data_name == 'weather':\n",
    "    load_data = np.load( './Data/datasets/weather.npy')\n",
    "\n",
    "elif data_name == 'MuJoCo':\n",
    "    load_data = np.load( './Data/datasets/MuJoCo.npy')\n",
    "    \n",
    "    \n",
    "np.random.shuffle(load_data)\n",
    "train_data = load_data\n",
    "\n",
    "Numble = train_data.shape[0]\n",
    "Length = train_data.shape[1]\n",
    "Feature = train_data.shape[2]\n",
    "Batchsize = 128\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X_data[idx], dtype=torch.float32) \n",
    "        return x   \n",
    "                    \n",
    "dataset = MyDataset(train_data)    \n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "def cycle(dl):         \n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "            \n",
    "dl = cycle( dataloader )\n",
    "print( next(dl).shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a215a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "464b30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = model.betas.device\n",
    "step = 0\n",
    "\n",
    "train_num_steps = 20000\n",
    "gradient_accumulate_every = config['solver']['gradient_accumulate_every'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3beb0331",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr = config['solver'].get('base_lr', 1.0e-4)\n",
    "ema_decay = config['solver']['ema']['decay']\n",
    "ema_update_every = config['solver']['ema']['update_interval']\n",
    "ema = EMA(model, beta=ema_decay, update_every=ema_update_every).to(device)\n",
    "opt = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=start_lr, betas=[0.9, 0.96])\n",
    "sc_cfg = config['solver']['scheduler']\n",
    "sc_cfg['params']['optimizer'] = opt   \n",
    "sch = instantiate_from_config(sc_cfg)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aedc9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce1994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe81c29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = instantiate_from_config(config['model']).cuda()\n",
    "\n",
    "model.load_state_dict(torch.load( \"./checkpoint/sp500.pth\"  ))\n",
    "ema.load_state_dict(torch.load( \"./checkpoint/sp500_ema.pth\"  ))\n",
    "\n",
    "# model.load_state_dict(torch.load( \"./checkpoint/ETTh1.pth\"  ))\n",
    "# ema.load_state_dict(torch.load( \"./checkpoint/ETTh1_ema.pth\"  ))\n",
    "\n",
    "# model.load_state_dict(torch.load( \"./checkpoint/ER.pth\"  ))\n",
    "# ema.load_state_dict(torch.load( \"./checkpoint/ER_ema.pth\"  ))\n",
    "\n",
    "\n",
    "# model.load_state_dict(torch.load( \"./checkpoint/Energy.pth\"  ))\n",
    "# ema.load_state_dict(torch.load( \"./checkpoint/Energy_ema.pth\"  ))\n",
    "\n",
    "# model.load_state_dict(torch.load( \"./checkpoint/weather.pth\"  ))\n",
    "# ema.load_state_dict(torch.load( \"./checkpoint/weather_ema.pth\"  ))\n",
    "\n",
    "# model.load_state_dict(torch.load( \"./checkpoint/MuJoCo.pth\"  ))\n",
    "# ema.load_state_dict(torch.load( \"./checkpoint/MuJoCo_ema.pth\"  ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d79d4ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5775, 30, 6)\n",
      "500\n",
      "torch.Size([500])\n",
      "tensor([8.7424e-05, 1.0685e-04, 1.2629e-04, 1.4572e-04, 1.6516e-04, 1.8459e-04,\n",
      "        2.0404e-04, 2.2348e-04, 2.4293e-04, 2.6239e-04], dtype=torch.float64)\n",
      "tensor([8.7424e-05, 1.0685e-04, 1.2629e-04, 1.4572e-04, 1.6516e-04, 1.8459e-04,\n",
      "        2.0404e-04, 2.2348e-04, 2.4293e-04, 2.6239e-04])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from Models.interpretable_diffusion.model_utils import default, identity, extract\n",
    "from Models.interpretable_diffusion.gaussian_diffusion import linear_beta_schedule, cosine_beta_schedule, Diffusion_TS\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "shape = (Numble, Length, Feature)\n",
    "print(shape)\n",
    "\n",
    "num_timesteps = model.num_timesteps\n",
    "print(num_timesteps)\n",
    "\n",
    "\n",
    "if beta_schedule == 'linear':\n",
    "    betas = linear_beta_schedule(num_timesteps)\n",
    "elif beta_schedule == 'cosine':\n",
    "    betas = cosine_beta_schedule(num_timesteps)\n",
    "            \n",
    "        \n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "print(betas.shape)\n",
    "\n",
    "print(betas[0:10])\n",
    "print(betas[0:10].to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2288ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = betas.to(torch.float32).cuda()\n",
    "alphas = alphas.to(torch.float32).cuda()\n",
    "alphas_cumprod = alphas_cumprod.to(torch.float32).cuda()\n",
    "alphas_cumprod_prev = alphas_cumprod_prev.to(torch.float32).cuda()\n",
    "\n",
    "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(torch.float32).cuda()\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod).to(torch.float32).cuda()\n",
    "log_one_minus_alphas_cumprod = torch.log(1. - alphas_cumprod).to(torch.float32).cuda()\n",
    "sqrt_recip_alphas_cumprod =  torch.sqrt(1. / alphas_cumprod).to(torch.float32).cuda()\n",
    "sqrt_recipm1_alphas_cumprod =  torch.sqrt(1. / alphas_cumprod - 1).to(torch.float32).cuda()\n",
    "\n",
    "\n",
    "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod).cuda()\n",
    "\n",
    "# above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "posterior_variance = posterior_variance.to(torch.float32).cuda()\n",
    "\n",
    "posterior_log_variance_clipped = torch.log(posterior_variance.clamp(min=1e-20)).to(torch.float32).cuda()\n",
    "posterior_mean_coef1 = betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod).to(torch.float32).cuda()\n",
    "posterior_mean_coef2 =  (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod).to(torch.float32).cuda()\n",
    "\n",
    "# calculate reweighting\n",
    "loss_weight =  ( torch.sqrt(alphas) * torch.sqrt(1. - alphas_cumprod) / betas / 100).to(torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1e7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796477d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2224786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduce missing data by removing the values around the mean\n",
      "(5775, 30, 6)\n",
      "0.15\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "\n",
    "class classify(nn.Module ):\n",
    "    def __init__(self, D):\n",
    "        super(classify, self).__init__()\n",
    "        self.logits_layer = nn.Sequential(\n",
    "                                          nn.Linear(D, D),  \n",
    "\n",
    "                                         nn.Tanh(),   \n",
    "        \n",
    "                                         nn.Linear(D, D),\n",
    "                                        )\n",
    "                                                                                \n",
    "    \n",
    "    def forward(self, l_out_mixed ):\n",
    "        \n",
    "        logits = self.logits_layer(l_out_mixed)       \n",
    "        p_s_given_x = Bernoulli(logits=logits)\n",
    "        \n",
    "        return p_s_given_x\n",
    "    \n",
    "def introduce_missing_superior_to_mean(X):\n",
    "    print(\"Introducing missing data with > mean\")\n",
    "    N, D = X.shape\n",
    "    Xnan = X.copy()\n",
    "\n",
    "    # ---- MNAR in D/2 dimensions\n",
    "    mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
    "    ix_larger_than_mean = Xnan[:, :int(D / 2)] > mean\n",
    "    Xnan[:, :int(D / 2)][ix_larger_than_mean] = np.nan\n",
    "\n",
    "    Xnan = Xnan.astype(np.float32)\n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan.astype(np.float32))] = 0\n",
    "\n",
    "    return Xnan, Xz\n",
    "\n",
    "\n",
    "def introduce_missing_mean_values(X, percentage_to_remove = 30):\n",
    "    print(\"Introduce missing data by removing the values around the mean\")\n",
    "    N, D = X.shape\n",
    "    Xnan = X.copy()\n",
    "\n",
    "    num_elements = int(N * percentage_to_remove / 100)   # number of elements to remove\n",
    "\n",
    "    # ---- MNAR in D/2 dimensions\n",
    "    mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
    "    abs_diff_from_mean = np.abs(Xnan[:, :int(D / 2)] - mean)\n",
    "    indices_to_remove = np.argsort(abs_diff_from_mean, axis = 0)[:num_elements]\n",
    "    # Set those values to NaN\n",
    "    for d in range(indices_to_remove.shape[1]):\n",
    "        Xnan[indices_to_remove[:, d], d] = np.nan\n",
    "    Xnan = Xnan.astype(np.float32)\n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan)] = 0\n",
    "    return Xnan, Xz\n",
    "\n",
    "\n",
    "def introduce_missing_extreme_values(X, percentile_extreme = 25):\n",
    "    print(\"Introducing missing data via removing extreme values\")\n",
    "    N, D = X.shape\n",
    "    Xnan = X.copy()\n",
    "\n",
    "    # ---- MNAR in D/2 dimensions\n",
    "    lower_bound = np.percentile(Xnan[:, :int(D / 2)], percentile_extreme, axis=0)\n",
    "    upper_bound = np.percentile(Xnan[:, :int(D / 2)], 100 - percentile_extreme, axis=0)\n",
    "\n",
    "    ix_lower = Xnan[:, :int(D / 2)] < lower_bound\n",
    "    ix_higher = Xnan[:, :int(D / 2)] > upper_bound\n",
    "    Xnan[:, :int(D / 2)][ix_lower | ix_higher] = np.nan  # 过大过小的去掉 换成nan\n",
    "    Xnan = Xnan.astype(np.float32)\n",
    "    \n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan)] = 0    # 换成0\n",
    "\n",
    "    return Xnan, Xz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_data = load_data.reshape(-1, Numble*Length,Feature).squeeze(0)\n",
    "\n",
    "Xnan, Xz = introduce_missing_mean_values( X_data)\n",
    "# Xnan, Xz = introduce_missing_superior_to_mean( X_data)\n",
    "# Xnan, Xz = introduce_missing_extreme_values( X_data)\n",
    "\n",
    "Xnan = Xnan.reshape(-1, Length, Feature)\n",
    "S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
    "print(S.shape)\n",
    "\n",
    "n_1 = np.sum(S == 1)\n",
    "n_0 = np.sum(S == 0)\n",
    "print(n_0/(n_1+n_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b39daaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5775, 30, 6]) torch.Size([5775, 30, 6])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(X_data.reshape(-1, Length, Feature), dtype=torch.float32, device=device) \n",
    "s = torch.tensor(S, dtype=torch.float32, device=device) \n",
    "print(x.shape, s.shape)\n",
    "\n",
    "train_dataset = TensorDataset(x, s ) \n",
    "train_loader  = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "classi = classify(D=Feature).to(device)\n",
    "optimizer = optim.Adam( list(classi.parameters()), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d932a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classi = classify(D=Feature).to(device)\n",
    "classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/sp500_rm_around_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/sp500_rm_superior_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/sp500_rm_extre_val.pth\"  ))\n",
    "\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/ETT_rm_around_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/ETT_rm_superior_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/ETT_rm_extre_val.pth\"  ))\n",
    "\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/ER_rm_around_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/ER_rm_superior_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/ER_rm_extre_val.pth\"  ))\n",
    "\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/Energy_rm_around_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/Energy_rm_superior_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/Energy_rm_extre_val.pth\"  ))\n",
    "\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/Weather_rm_around_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/Weather_rm_superior_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/Weather_rm_extre_val.pth\"  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "236f5247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC123\\AppData\\Local\\Temp\\ipykernel_324\\2098842402.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_s = torch.tensor(s, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5775, 30, 6]) torch.Size([5775, 30, 6])\n",
      "tensor(0.9928, device='cuda:0', grad_fn=<MeanBackward0>) torch.float32\n"
     ]
    }
   ],
   "source": [
    "test_data = torch.tensor(X_data.reshape(-1,Length, Feature), dtype=torch.float32, device=device) \n",
    "test_s = torch.tensor(s, dtype=torch.float32, device=device)\n",
    "print(test_data.shape, test_s.shape)\n",
    "\n",
    "\n",
    "p_s_given_x = classi(test_data)\n",
    "p = p_s_given_x.log_prob(test_s).exp()\n",
    "print(p.mean(), test_s.dtype)\n",
    "\n",
    "mask_data = S.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2745c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([500, 30, 6])\n",
      "torch.Size([500, 30, 6])\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X_data[idx], dtype=torch.float32)    \n",
    "        y = torch.tensor(self.Y_data[idx])                         \n",
    "        \n",
    "\n",
    "        return x, y   \n",
    "             \n",
    "dataset_with_mask = MyDataset(load_data[0:500], mask_data[0:500] )    \n",
    "\n",
    "dataloader_with_mask = DataLoader(dataset_with_mask, batch_size=500, shuffle=True)\n",
    "print( len( next(iter(dataloader_with_mask)) ) )\n",
    "print(next(iter(dataloader_with_mask))[0].shape)\n",
    "print(next(iter(dataloader_with_mask))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba52d5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DPS loop: 500it [00:14, 33.52it/s]\n"
     ]
    }
   ],
   "source": [
    "'变分法'\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "sampling_steps =  num_timesteps\n",
    "shape = (Length, Feature)\n",
    "padding_masks = None\n",
    "clip_x_start = False\n",
    "clip_denoised = True\n",
    "\n",
    "\n",
    "samples = np.empty([0, shape[0], shape[1]])\n",
    "reals = np.empty([0, shape[0], shape[1]])\n",
    "masks = np.empty([0, shape[0], shape[1]])    \n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "ema.ema_model.model.eval()\n",
    "classi.eval()\n",
    "\n",
    "\n",
    "loss1_list = []\n",
    "loss2_list = []\n",
    "loss3_list = []\n",
    "loss_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for idx, (x, t_m) in  enumerate(dataloader_with_mask):     \n",
    "    x, t_m = x.to(device), t_m.to(device)\n",
    "    partial_mask = t_m   \n",
    "    target = x*t_m                                          \n",
    "\n",
    "\n",
    "    initial = torch.randn_like(x) \n",
    "    temp = copy.deepcopy(initial)\n",
    "\n",
    "\n",
    "    mu = torch.autograd.Variable(temp, requires_grad=True)  \n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam([mu], lr=0.01, betas=(0.9, 0.99), weight_decay=0.0)   \n",
    "\n",
    "    for t in tqdm(reversed(range(0, num_timesteps)), desc='DPS loop'):   \n",
    "\n",
    "\n",
    "        ori_t = t\n",
    "        batched_times = torch.full((x.shape[0],), t, device=x.device, dtype=torch.long)\n",
    "        t = batched_times\n",
    "\n",
    "        if padding_masks is None:\n",
    "            padding_masks = torch.ones(x.shape[0], Length, dtype=bool, device=x.device)\n",
    "\n",
    "        maybe_clip = partial(torch.clamp, min=-1., max=1.) if clip_x_start else identity\n",
    "\n",
    "           \n",
    "        noise_x0 = torch.randn_like(mu)\n",
    "        noise_xt = torch.randn_like(mu)\n",
    "        \n",
    "        sigma_x0 = 0.001\n",
    "        \n",
    "        x0_pred = mu + sigma_x0 * noise_x0           \n",
    "        \n",
    "        xt = sqrt_alphas_cumprod[ori_t] * x0_pred + sqrt_one_minus_alphas_cumprod[ori_t] * noise_xt \n",
    "\n",
    "        trend, season = ema.ema_model.model(xt, t, padding_masks=padding_masks)   \n",
    "        x_start = trend + season \n",
    "        x_start = maybe_clip(x_start)\n",
    "        pred_noise = ( extract(sqrt_recip_alphas_cumprod, t, xt.shape ) * xt - x_start) /  extract(sqrt_recipm1_alphas_cumprod, t, xt.shape) \n",
    "\n",
    "        \n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "               \n",
    "        mask_temp = partial_mask.to(torch.float32)  \n",
    "\n",
    "        e1 =  betas[ori_t] / 2 * sqrt_one_minus_alphas_cumprod[ori_t]\n",
    "\n",
    "    \n",
    "        loss1 =  e1 * torch.linalg.norm( pred_noise - noise_xt)\n",
    "        \n",
    "        loss2 = x0_pred * mask_temp - target* mask_temp      \n",
    "        loss2 = torch.linalg.norm( loss2 )\n",
    "        \n",
    "        p_s_given_x = classi(x0_pred)\n",
    "        p = p_s_given_x.log_prob( mask_temp )                     \n",
    "        loss3 = ( p* (1-mask_temp) ).sum()/(1-mask_temp).sum()        \n",
    "\n",
    "        \n",
    "    \n",
    "        '========= choose the hyper-parameters according to table ======= '\n",
    "        loss =   0.5  * loss1 +  1  * loss2  -   0.0001 * loss3 \n",
    "\n",
    "        \n",
    "        \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        loss1_list.append(loss1.item() )\n",
    "        loss2_list.append(loss2.item() )\n",
    "        loss3_list.append(loss3.item() )\n",
    "        loss_list.append(loss.item() )\n",
    "        \n",
    "\n",
    "    x0_pred.detach()\n",
    "\n",
    "    x0_pred[partial_mask] = target[partial_mask]   #\n",
    "\n",
    "    samples = np.row_stack([samples, x0_pred.detach().cpu().numpy()])\n",
    "    reals = np.row_stack([reals, x.detach().cpu().numpy()])\n",
    "    masks = np.row_stack([masks, t_m.detach().cpu().numpy()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c540f5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0066 0.0047\n",
      "(500, 30, 6)\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import calc_mse,calc_rmse,calc_mae\n",
    "\n",
    "rmse = calc_rmse(samples, reals, 1-masks)\n",
    "mae = calc_mae(samples, reals, 1-masks)  \n",
    "\n",
    "print(round(rmse,4), round(mae,4) )\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d9871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322cc17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c88e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f54c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a046828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
