{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7cca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from engine.logger import Logger\n",
    "from engine.solver import Trainer\n",
    "from Data.build_dataloader import build_dataloader, build_dataloader_cond\n",
    "from Models.interpretable_diffusion.model_utils import unnormalize_to_zero_to_one\n",
    "from Utils.io_utils import load_yaml_config, seed_everything, merge_opts_to_config, instantiate_from_config\n",
    "from Utils.metric_utils import visualization\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from ema_pytorch import EMA\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from Utils.io_utils import instantiate_from_config, get_model_parameters_info\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font',family='Times New Roman') \n",
    "\n",
    "import umap                        \n",
    "import seaborn as sns \n",
    "from tqdm import tqdm\n",
    "from einops import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a88cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Training Script')\n",
    "    parser.add_argument('--name', type=str, default=None)\n",
    "\n",
    "    parser.add_argument('--config_file', type=str, default=None, \n",
    "                        help='path of config file')\n",
    "    parser.add_argument('--output', type=str, default='OUTPUT', \n",
    "                        help='directory to save the results')\n",
    "    parser.add_argument('--tensorboard', action='store_true', \n",
    "                        help='use tensorboard for logging')\n",
    "\n",
    "    # args for random\n",
    "\n",
    "    parser.add_argument('--cudnn_deterministic', action='store_true', default=False,\n",
    "                        help='set cudnn.deterministic True')\n",
    "    parser.add_argument('--seed', type=int, default=10, \n",
    "                        help='seed for initializing training.')\n",
    "    parser.add_argument('--gpu', type=int, default=None,\n",
    "                        help='GPU id to use. If given, only the specific gpu will be'\n",
    "                        ' used, and ddp will be disabled')\n",
    "    \n",
    "    # args for training\n",
    "    parser.add_argument('--train', action='store_true', default=False, help='Train or Test.')\n",
    "    parser.add_argument('--sample', type=int, default=0, \n",
    "                        choices=[0, 1], help='Condition or Uncondition.')\n",
    "    parser.add_argument('--mode', type=str, default='infill',\n",
    "                        help='Infilling or Forecasting.')\n",
    "    parser.add_argument('--milestone', type=int, default=10)\n",
    "\n",
    "    parser.add_argument('--missing_ratio', type=float, default=0., help='Ratio of Missing Values.')\n",
    "    parser.add_argument('--pred_len', type=int, default=0, help='Length of Predictions.')\n",
    "    \n",
    "    \n",
    "    # args for modify config\n",
    "    parser.add_argument('opts', help='Modify config options using the command-line',\n",
    "                        default=None, nargs=argparse.REMAINDER)  \n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.save_dir = os.path.join(args.output, f'{args.name}')\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a503d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(name=None, config_file='./Config/SP500.yaml', output='OUTPUT', tensorboard=False, cudnn_deterministic=False, seed=10, gpu=0, train=True, sample=0, mode='infill', milestone=10, missing_ratio=0.0, pred_len=0, opts=[], save_dir='OUTPUT\\\\None')\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "\n",
    "data_name = 'SP500'\n",
    "# data_name = 'ETTh1'\n",
    "# data_name = 'ER'\n",
    "# data_name = 'Energy'\n",
    "# data_name = 'weather'\n",
    "\n",
    "\n",
    "if data_name == 'SP500':\n",
    "    args.config_file =  './Config/SP500.yaml'\n",
    "\n",
    "elif data_name == 'ER':\n",
    "    args.config_file =  './Config/exchange_rate.yaml'\n",
    "    \n",
    "elif data_name == 'ETTh1':\n",
    "    args.config_file =  './Config/ETTh1.yaml'\n",
    "\n",
    "elif data_name == 'Energy':\n",
    "    args.config_file =  './Config/Energy.yaml'\n",
    "    \n",
    "elif data_name == 'weather':\n",
    "    args.config_file =  './Config/weather.yaml'\n",
    "    \n",
    "elif data_name == 'MuJoCo':\n",
    "    args.config_file =  './Config/MuJoCo.yaml'\n",
    "    \n",
    "args.gpu =  0\n",
    "args.train = True\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3918163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 10\n",
      "{'model': {'target': 'Models.interpretable_diffusion.gaussian_diffusion.Diffusion_TS', 'params': {'seq_length': 30, 'feature_size': 6, 'n_layer_enc': 2, 'n_layer_dec': 2, 'd_model': 64, 'timesteps': 500, 'sampling_timesteps': 500, 'loss_type': 'l1', 'beta_schedule': 'cosine', 'n_heads': 4, 'mlp_hidden_times': 4, 'attn_pd': 0.0, 'resid_pd': 0.0, 'kernel_size': 1, 'padding_size': 0}}, 'solver': {'base_lr': 1e-05, 'max_epochs': 10000, 'results_folder': './Checkpoints_stock', 'gradient_accumulate_every': 2, 'save_cycle': 500, 'ema': {'decay': 0.995, 'update_interval': 10}, 'scheduler': {'target': 'engine.lr_sch.ReduceLROnPlateauWithWarmup', 'params': {'factor': 0.5, 'patience': 2000, 'min_lr': 1e-05, 'threshold': 0.1, 'threshold_mode': 'rel', 'warmup_lr': 0.0008, 'warmup': 500, 'verbose': False}}}, 'dataloader': {'train_dataset': {'target': 'Utils.Data_utils.real_datasets.CustomDataset', 'params': {'name': 'stock', 'proportion': 1.0, 'data_root': './Data/datasets/stock_data.csv', 'window': 24, 'save2npy': True, 'neg_one_to_one': True, 'seed': 123, 'period': 'train'}}, 'test_dataset': {'target': 'Utils.Data_utils.real_datasets.CustomDataset', 'params': {'name': 'stock', 'proportion': 0.9, 'data_root': './Data/datasets/stock_data.csv', 'window': 24, 'save2npy': True, 'neg_one_to_one': True, 'seed': 123, 'period': 'test', 'style': 'separate', 'distribution': 'geometric'}, 'coefficient': 0.01, 'step_size': 0.05, 'sampling_steps': 200}, 'batch_size': 128, 'sample_size': 256, 'shuffle': True}}\n",
      "cosine 500\n"
     ]
    }
   ],
   "source": [
    "if args.seed is not None:\n",
    "    seed_everything(args.seed)\n",
    "\n",
    "if args.gpu is not None:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "config = load_yaml_config(args.config_file)\n",
    "config = merge_opts_to_config(config, args.opts)\n",
    "\n",
    "\n",
    "config['dataloader']['batch_size'] = 128\n",
    "print(config)\n",
    "\n",
    "beta_schedule = config['model']['params']['beta_schedule'] \n",
    "timesteps = config['model']['params']['timesteps'] \n",
    "print(beta_schedule, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f1c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(args)\n",
    "logger.save_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4efbc40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 500 l1\n",
      "500 False\n"
     ]
    }
   ],
   "source": [
    "model = instantiate_from_config(config['model']).cuda()\n",
    "# print(model)\n",
    "print(model.use_ff, model.num_timesteps, model.loss_type)\n",
    "print(model.sampling_timesteps, model.fast_sampling)\n",
    "# print(model.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39db9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3406e18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30, 6])\n",
      "5775\n",
      "[0.19777922 0.19484571 0.0380807  0.19849203 0.19913802 0.18855503]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if data_name == 'SP500':\n",
    "    load_data = np.load( './Data/datasets/sp500.npy')\n",
    "    \n",
    "elif data_name == 'ETTh1':\n",
    "    load_data = np.load( './Data/datasets/ETTh1.npy')\n",
    "    \n",
    "elif data_name == 'ER':\n",
    "    load_data = np.load( './Data/datasets/exchange_rate.npy')\n",
    "\n",
    "elif data_name == 'Energy':\n",
    "    load_data = np.load( './Data/datasets/Energy.npy')\n",
    "    \n",
    "elif data_name == 'weather':\n",
    "    load_data = np.load( './Data/datasets/weather.npy')\n",
    "\n",
    "elif data_name == 'MuJoCo':\n",
    "    load_data = np.load( './Data/datasets/MuJoCo.npy')\n",
    "    \n",
    "    \n",
    "\n",
    "np.random.shuffle(load_data)\n",
    "train_data = load_data\n",
    "\n",
    "\n",
    "Numble = train_data.shape[0]\n",
    "Length = train_data.shape[1]\n",
    "Feature = train_data.shape[2]\n",
    "Batchsize = 128\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X_data[idx], dtype=torch.float32)  \n",
    "        return x   \n",
    "                    \n",
    "dataset = MyDataset(train_data)    \n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "def cycle(dl):         \n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "            \n",
    "dl = cycle( dataloader )\n",
    "print( next(dl).shape )\n",
    "print(Numble)\n",
    "\n",
    "print( load_data[0][0]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a215a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "464b30aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19777922 0.19484571 0.0380807  0.19849203 0.19913802 0.18855503]\n"
     ]
    }
   ],
   "source": [
    "device = model.betas.device\n",
    "step = 0\n",
    "\n",
    "train_num_steps = 20000\n",
    "gradient_accumulate_every = config['solver']['gradient_accumulate_every'] \n",
    "\n",
    "print( load_data[0][0]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3beb0331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19777922 0.19484571 0.0380807  0.19849203 0.19913802 0.18855503]\n"
     ]
    }
   ],
   "source": [
    "start_lr = config['solver'].get('base_lr', 1.0e-4)\n",
    "ema_decay = config['solver']['ema']['decay']\n",
    "ema_update_every = config['solver']['ema']['update_interval']\n",
    "ema = EMA(model, beta=ema_decay, update_every=ema_update_every).to(device)\n",
    "\n",
    "opt = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=start_lr, betas=[0.9, 0.96])\n",
    "# print(opt)\n",
    "\n",
    "sc_cfg = config['solver']['scheduler']\n",
    "sc_cfg['params']['optimizer'] = opt   \n",
    "sch = instantiate_from_config(sc_cfg)  \n",
    "\n",
    "print( load_data[0][0]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aedc9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce1994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe81c29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = instantiate_from_config(config['model']).cuda()\n",
    "\n",
    "model.load_state_dict(torch.load( \"./checkpoint/sp500.pth\"  ))\n",
    "ema.load_state_dict(torch.load( \"./checkpoint/sp500_ema.pth\"  ))\n",
    "\n",
    "# model.load_state_dict(torch.load( \"./checkpoint/ETTh1.pth\"  ))\n",
    "# ema.load_state_dict(torch.load( \"./checkpoint/ETTh1_ema.pth\"  ))\n",
    "\n",
    "# model.load_state_dict(torch.load( \"./checkpoint/ER.pth\"  ))\n",
    "# ema.load_state_dict(torch.load( \"./checkpoint/ER_ema.pth\"  ))\n",
    "\n",
    "# model.load_state_dict(torch.load( \"./checkpoint/Energy.pth\"  ))\n",
    "# ema.load_state_dict(torch.load( \"./checkpoint/Energy_ema.pth\"  ))\n",
    "\n",
    "# model.load_state_dict(torch.load( \"./checkpoint/weather.pth\"  ))\n",
    "# ema.load_state_dict(torch.load( \"./checkpoint/weather_ema.pth\"  ))\n",
    "\n",
    "# model.load_state_dict(torch.load( \"./checkpoint/MuJoCo.pth\"  ))\n",
    "# ema.load_state_dict(torch.load( \"./checkpoint/MuJoCo_ema.pth\"  ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d79d4ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5775, 30, 6)\n",
      "500\n",
      "torch.Size([500])\n",
      "tensor([8.7424e-05, 1.0685e-04, 1.2629e-04, 1.4572e-04, 1.6516e-04, 1.8459e-04,\n",
      "        2.0404e-04, 2.2348e-04, 2.4293e-04, 2.6239e-04], dtype=torch.float64)\n",
      "tensor([8.7424e-05, 1.0685e-04, 1.2629e-04, 1.4572e-04, 1.6516e-04, 1.8459e-04,\n",
      "        2.0404e-04, 2.2348e-04, 2.4293e-04, 2.6239e-04])\n"
     ]
    }
   ],
   "source": [
    "from Models.interpretable_diffusion.model_utils import default, identity, extract\n",
    "from Models.interpretable_diffusion.gaussian_diffusion import linear_beta_schedule, cosine_beta_schedule, Diffusion_TS\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "shape = (Numble, Length, Feature)\n",
    "print(shape)\n",
    "\n",
    "num_timesteps = model.num_timesteps\n",
    "print(num_timesteps)\n",
    "\n",
    "\n",
    "if beta_schedule == 'linear':\n",
    "    betas = linear_beta_schedule(num_timesteps)\n",
    "elif beta_schedule == 'cosine':\n",
    "    betas = cosine_beta_schedule(num_timesteps)\n",
    "            \n",
    "        \n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "print(betas.shape)\n",
    "\n",
    "print(betas[0:10])\n",
    "print(betas[0:10].to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b796477d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500]) torch.Size([500])\n"
     ]
    }
   ],
   "source": [
    "betas = betas.to(torch.float32).cuda()\n",
    "alphas = alphas.to(torch.float32).cuda()\n",
    "alphas_cumprod = alphas_cumprod.to(torch.float32).cuda()\n",
    "alphas_cumprod_prev = alphas_cumprod_prev.to(torch.float32).cuda()\n",
    "\n",
    "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(torch.float32).cuda()\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod).to(torch.float32).cuda()\n",
    "log_one_minus_alphas_cumprod = torch.log(1. - alphas_cumprod).to(torch.float32).cuda()\n",
    "sqrt_recip_alphas_cumprod =  torch.sqrt(1. / alphas_cumprod).to(torch.float32).cuda()\n",
    "sqrt_recipm1_alphas_cumprod =  torch.sqrt(1. / alphas_cumprod - 1).to(torch.float32).cuda()\n",
    "\n",
    "\n",
    "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod).cuda()\n",
    "\n",
    "# above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "posterior_variance = posterior_variance.to(torch.float32).cuda()\n",
    "\n",
    "posterior_log_variance_clipped = torch.log(posterior_variance.clamp(min=1e-20)).to(torch.float32).cuda()\n",
    "posterior_mean_coef1 = betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod).to(torch.float32).cuda()\n",
    "posterior_mean_coef2 =  (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod).to(torch.float32).cuda()\n",
    "\n",
    "# calculate reweighting\n",
    "loss_weight =  ( torch.sqrt(alphas) * torch.sqrt(1. - alphas_cumprod) / betas / 100).to(torch.float32).cuda()\n",
    "\n",
    "\n",
    "print(sqrt_alphas_cumprod.shape, posterior_variance.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2224786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19777922 0.19484571 0.0380807  0.19849203 0.19913802 0.18855503]\n",
      "Introduce missing data by removing the values around the mean\n",
      "(5775, 30, 6)\n",
      "0.15\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "\n",
    "class classify(nn.Module ):\n",
    "    def __init__(self, D):\n",
    "        super(classify, self).__init__()\n",
    "        self.logits_layer = nn.Sequential(\n",
    "                                          nn.Linear(D, D),  \n",
    "                                         nn.Tanh(),  \n",
    "            \n",
    "                                         nn.Linear(D, D),\n",
    "                                        )\n",
    "                                                                                \n",
    "    \n",
    "    def forward(self, l_out_mixed ):\n",
    "        \n",
    "        logits = self.logits_layer(l_out_mixed)      \n",
    "        p_s_given_x = Bernoulli(logits=logits)\n",
    "        \n",
    "        return p_s_given_x\n",
    "    \n",
    "def introduce_missing_superior_to_mean(X):\n",
    "    print(\"Introducing missing data with > mean\")\n",
    "    N, D = X.shape\n",
    "    Xnan = X.copy()\n",
    "\n",
    "    # ---- MNAR in D/2 dimensions\n",
    "    mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
    "    ix_larger_than_mean = Xnan[:, :int(D / 2)] > mean\n",
    "    Xnan[:, :int(D / 2)][ix_larger_than_mean] = np.nan\n",
    "\n",
    "    Xnan = Xnan.astype(np.float32)\n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan.astype(np.float32))] = 0\n",
    "\n",
    "    return Xnan, Xz\n",
    "\n",
    "\n",
    "def introduce_missing_mean_values(X, percentage_to_remove = 30):\n",
    "    print(\"Introduce missing data by removing the values around the mean\")\n",
    "    N, D = X.shape\n",
    "    Xnan = X.copy()\n",
    "\n",
    "    num_elements = int(N * percentage_to_remove / 100)   # number of elements to remove\n",
    "\n",
    "    # ---- MNAR in D/2 dimensions\n",
    "    mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
    "    abs_diff_from_mean = np.abs(Xnan[:, :int(D / 2)] - mean)\n",
    "    indices_to_remove = np.argsort(abs_diff_from_mean, axis = 0)[:num_elements]\n",
    "    # Set those values to NaN\n",
    "    for d in range(indices_to_remove.shape[1]):\n",
    "        Xnan[indices_to_remove[:, d], d] = np.nan\n",
    "    Xnan = Xnan.astype(np.float32)\n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan)] = 0\n",
    "    return Xnan, Xz\n",
    "\n",
    "\n",
    "def introduce_missing_extreme_values(X, percentile_extreme = 25):\n",
    "    print(\"Introducing missing data via removing extreme values\")\n",
    "    N, D = X.shape\n",
    "    Xnan = X.copy()\n",
    "\n",
    "    # ---- MNAR in D/2 dimensions\n",
    "    lower_bound = np.percentile(Xnan[:, :int(D / 2)], percentile_extreme, axis=0)\n",
    "    upper_bound = np.percentile(Xnan[:, :int(D / 2)], 100 - percentile_extreme, axis=0)\n",
    "\n",
    "    ix_lower = Xnan[:, :int(D / 2)] < lower_bound\n",
    "    ix_higher = Xnan[:, :int(D / 2)] > upper_bound\n",
    "    Xnan[:, :int(D / 2)][ix_lower | ix_higher] = np.nan  \n",
    "    Xnan = Xnan.astype(np.float32)\n",
    "    \n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan)] = 0   \n",
    "\n",
    "    return Xnan, Xz\n",
    "\n",
    "\n",
    "\n",
    "print( load_data[0][0] )\n",
    "\n",
    "X_data = load_data.reshape(-1, Numble*Length,Feature).squeeze(0)\n",
    "\n",
    "\n",
    "Xnan, Xz = introduce_missing_mean_values( X_data)\n",
    "# Xnan, Xz = introduce_missing_superior_to_mean( X_data)\n",
    "# Xnan, Xz = introduce_missing_extreme_values( X_data)\n",
    "\n",
    "Xnan = Xnan.reshape(-1, Length, Feature)\n",
    "S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
    "print(S.shape)\n",
    "\n",
    "n_1 = np.sum(S == 1)\n",
    "n_0 = np.sum(S == 0)\n",
    "print(n_0/(n_1+n_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b39daaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5775, 30, 6]) torch.Size([5775, 30, 6])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(X_data.reshape(-1, Length, Feature), dtype=torch.float32, device=device) \n",
    "s = torch.tensor(S, dtype=torch.float32, device=device) \n",
    "print(x.shape, s.shape)\n",
    "\n",
    "train_dataset = TensorDataset(x, s ) \n",
    "train_loader  = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "classi = classify(D=Feature).to(device)\n",
    "optimizer = optim.Adam( list(classi.parameters()), lr=0.0005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20548324",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# train MNAR classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "270ae329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# losslist1 = []\n",
    "# losslist2 = []\n",
    "\n",
    "# for i in tqdm( range(30000) ):\n",
    "    \n",
    "#     for x_train, s_train in train_loader:   # [1000, 24, 8]\n",
    "\n",
    "#         p_s_given_x = classi(x_train)\n",
    "        \n",
    "        \n",
    "#         log_p_s_given_x =  p_s_given_x.log_prob( s_train )\n",
    "\n",
    "\n",
    "#         '用第一个loss效果好一些 最终应该差不多'\n",
    "#         loss = -torch.mean(log_p_s_given_x)\n",
    "\n",
    "#         losslist1.append(loss.item())\n",
    "#         losslist2.append(log_p_s_given_x.mean().item())\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "        \n",
    "# plt.figure(figsize=(5, 3))\n",
    "# plt.plot(losslist1, label='loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# plt.plot(losslist2, label='log_p_s_given_x')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ae32e",
   "metadata": {},
   "source": [
    "# load trained MNAR classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d932a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classi = classify(D=Feature).to(device)\n",
    "classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/sp500_rm_around_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/sp500_rm_superior_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/sp500_rm_extre_val.pth\"  ))\n",
    "\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/ETT_rm_around_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/ETT_rm_superior_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/ETT_rm_extre_val.pth\"  ))\n",
    "\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/ER_rm_around_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/ER_rm_superior_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/ER_rm_extre_val.pth\"  ))\n",
    "\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/Energy_rm_around_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/Energy_rm_superior_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/Energy_rm_extre_val.pth\"  ))\n",
    "\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/Weather_rm_around_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/Weather_rm_superior_mean_val.pth\"  ))\n",
    "# classi.load_state_dict(torch.load( \"./checkpoint/mnar_classifier/Weather_rm_extre_val.pth\"  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "236f5247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC123\\AppData\\Local\\Temp\\ipykernel_4804\\2098842402.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_s = torch.tensor(s, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5775, 30, 6]) torch.Size([5775, 30, 6])\n",
      "tensor(0.9928, device='cuda:0', grad_fn=<MeanBackward0>) torch.float32\n"
     ]
    }
   ],
   "source": [
    "test_data = torch.tensor(X_data.reshape(-1,Length, Feature), dtype=torch.float32, device=device) \n",
    "test_s = torch.tensor(s, dtype=torch.float32, device=device)\n",
    "print(test_data.shape, test_s.shape)\n",
    "\n",
    "\n",
    "p_s_given_x = classi(test_data)\n",
    "p = p_s_given_x.log_prob(test_s).exp()\n",
    "print(p.mean(), test_s.dtype)\n",
    "\n",
    "mask_data = S.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2745c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([500, 30, 6])\n",
      "torch.Size([500, 30, 6])\n",
      "tensor([0.0757, 0.0740, 0.0512, 0.0807, 0.0792, 0.0679])\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X_data[idx], dtype=torch.float32)    \n",
    "        y = torch.tensor(self.Y_data[idx])                         \n",
    "        \n",
    "        return x, y   \n",
    "    \n",
    "\n",
    "\n",
    "dataset_with_mask = MyDataset(load_data[0:500], mask_data[0:500] ) \n",
    "dataloader_with_mask = DataLoader(dataset_with_mask, batch_size=500, shuffle=True)  \n",
    "print( len( next(iter(dataloader_with_mask)) ) )\n",
    "print(next(iter(dataloader_with_mask))[0].shape)\n",
    "print(next(iter(dataloader_with_mask))[1].shape)\n",
    "torch.manual_seed(0)\n",
    "print(next(iter(dataloader_with_mask))[0][0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22506b00",
   "metadata": {},
   "source": [
    "# DPS-not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bc4753f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DPS loop: 500it [00:18, 27.03it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampling_steps =  num_timesteps\n",
    "shape = (Length, Feature)\n",
    "padding_masks = None\n",
    "clip_x_start = False\n",
    "clip_denoised = True\n",
    "\n",
    "samples = np.empty([0, shape[0], shape[1]])\n",
    "reals = np.empty([0, shape[0], shape[1]])\n",
    "masks = np.empty([0, shape[0], shape[1]])    \n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "ema.ema_model.model.eval()\n",
    "classi.eval()\n",
    "\n",
    "\n",
    "for idx, (x, t_m) in  enumerate(dataloader_with_mask):     #\n",
    "    x, t_m = x.to(device), t_m.to(device)\n",
    "    partial_mask = t_m\n",
    "    target = x*t_m                                     \n",
    "    xt1 = torch.randn(x.shape, device=device)                                      \n",
    "    xt = xt1 \n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    for t in tqdm(reversed(range(0, num_timesteps)), desc='DPS loop'):\n",
    "\n",
    "        xt = xt.requires_grad_()      \n",
    "\n",
    "        ori_t = t\n",
    "        batched_times = torch.full((xt.shape[0],), t, device=x.device, dtype=torch.long)\n",
    "        t = batched_times\n",
    "\n",
    "        if padding_masks is None:\n",
    "            padding_masks = torch.ones(x.shape[0], Length, dtype=bool, device=x.device)\n",
    "\n",
    "        maybe_clip = partial(torch.clamp, min=-1., max=1.) if clip_x_start else identity\n",
    "\n",
    "        trend, season = ema.ema_model.model(xt, t, padding_masks=padding_masks)   \n",
    "        x_start = trend + season \n",
    "        x_start = maybe_clip(x_start)\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        mean = ( extract( posterior_mean_coef1, t, xt.shape) * x_start +\n",
    "                 extract( posterior_mean_coef2, t, xt.shape) * xt )\n",
    "\n",
    "        variance = extract( posterior_variance, t, xt.shape)  \n",
    "        log_variance_clipped = extract( posterior_log_variance_clipped, t, xt.shape)\n",
    "\n",
    "\n",
    "        noise = torch.randn_like(xt) if ori_t > 0 else 0.       \n",
    "        sigma = (0.5 * log_variance_clipped).exp()\n",
    "        xt_1 = mean + sigma * noise                             \n",
    "\n",
    "\n",
    "\n",
    "        mask_temp = partial_mask.to(torch.float32)\n",
    "\n",
    "        difference = x_start * mask_temp - target * mask_temp  \n",
    "        infill_loss = torch.linalg.norm(difference)\n",
    "        norm_grad = torch.autograd.grad(outputs=infill_loss, inputs=xt, retain_graph=True)[0]\n",
    "\n",
    "\n",
    "        x_temp = x_start * (1-mask_temp) + target * mask_temp  \n",
    "        p_s_given_x = classi(x_start)\n",
    "        p =  p_s_given_x.log_prob( mask_temp )                    \n",
    "        p = ( p* (1-mask_temp) ).sum()/(1-mask_temp).sum()        \n",
    "        norm_grad1 = torch.autograd.grad(outputs=p, inputs=xt)[0]  \n",
    "\n",
    "\n",
    "        '========= choose the hyper-parameters according to table ======= '\n",
    "        \n",
    "        xt_1 = xt_1 -  0.6   * norm_grad  +   0.3  * norm_grad1\n",
    "    \n",
    "#         xt_1 = xt_1 -  0.6 * norm_grad  \n",
    "\n",
    "\n",
    "        xt = xt_1.detach_() \n",
    "\n",
    "\n",
    "    xt[partial_mask] = target[partial_mask]  \n",
    "    samples = np.row_stack([samples, xt.detach().cpu().numpy()])\n",
    "    reals = np.row_stack([reals, x.detach().cpu().numpy()])\n",
    "    masks = np.row_stack([masks, t_m.detach().cpu().numpy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "501431d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0087 0.0061\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import calc_mse,calc_rmse,calc_mae\n",
    "rmse = calc_rmse(samples, reals, 1-masks)\n",
    "mae = calc_mae(samples, reals, 1-masks)  \n",
    "print(round(rmse,4), round(mae,4) )\n",
    "\n",
    "\n",
    "# data =( samples, reals,masks )\n",
    "# np.save('./result_npy/DPS_not_SP500_case1.npy',data)\n",
    "# np.save('./result_npy/DPS_not_ETT_case1.npy',data)\n",
    "# np.save('./result_npy/DPS_not_ER_case1.npy',data)\n",
    "# np.save('./result_npy/DPS_not_Energy_case1.npy',data)\n",
    "# np.save('./result_npy/DPS_not_Weather_case1.npy',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264c430",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Diffusion-TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11dd2d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DPS loop: 500it [00:21, 23.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "coef = 1.0e-2\n",
    "coef_=0.\n",
    "step_size = 5.0e-2\n",
    "learning_rate = step_size\n",
    "sampling_steps =  num_timesteps\n",
    "shape = (Length, Feature)\n",
    "padding_masks = None\n",
    "clip_x_start = False\n",
    "clip_denoised = True\n",
    "\n",
    "samples = np.empty([0, shape[0], shape[1]])\n",
    "reals = np.empty([0, shape[0], shape[1]])\n",
    "masks = np.empty([0, shape[0], shape[1]])    \n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "with torch.no_grad(): \n",
    "    \n",
    "    for idx, (x, t_m) in  enumerate(dataloader_with_mask):     \n",
    "        x, t_m = x.to(device), t_m.to(device)\n",
    "        partial_mask = t_m\n",
    "        target = x*t_m\n",
    "                                                  \n",
    "        xt = torch.randn(x.shape, device=device)   \n",
    "\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "    \n",
    "        for t in tqdm(reversed(range(0, num_timesteps)), desc='DPS loop'):\n",
    "\n",
    "            ori_t = t\n",
    "            batched_times = torch.full((xt.shape[0],), t, device=x.device, dtype=torch.long)\n",
    "            t = batched_times\n",
    "\n",
    "            if padding_masks is None:\n",
    "                padding_masks = torch.ones(x.shape[0], Length, dtype=bool, device=x.device)\n",
    "\n",
    "            maybe_clip = partial(torch.clamp, min=-1., max=1.) if clip_x_start else identity\n",
    "\n",
    "            trend, season = ema.ema_model.model(xt, t, padding_masks=padding_masks)   \n",
    "            x_start = trend + season \n",
    "            x_start = maybe_clip(x_start)\n",
    "\n",
    "\n",
    "            if clip_denoised:\n",
    "                x_start.clamp_(-1., 1.)\n",
    "\n",
    "\n",
    "            mean = ( extract( posterior_mean_coef1, t, xt.shape) * x_start +\n",
    "                     extract( posterior_mean_coef2, t, xt.shape) * xt )\n",
    "\n",
    "            variance = extract( posterior_variance, t, xt.shape) \n",
    "            log_variance_clipped = extract( posterior_log_variance_clipped, t, xt.shape)\n",
    "\n",
    "\n",
    "\n",
    "            noise = torch.randn_like(xt) if ori_t > 0 else 0.       \n",
    "            sigma = (0.5 * log_variance_clipped).exp()\n",
    "            xt = mean + sigma * noise                             \n",
    "\n",
    "\n",
    "        \n",
    "            if t[0].item() < num_timesteps * 0.05:   \n",
    "                K = 0\n",
    "            elif t[0].item() > num_timesteps * 0.9:\n",
    "                K = 3\n",
    "            elif t[0].item() > num_timesteps * 0.75:\n",
    "                K = 2\n",
    "                learning_rate = learning_rate * 0.5   \n",
    "            else:\n",
    "                K = 1\n",
    "                learning_rate = learning_rate * 0.25\n",
    "\n",
    "            input_embs_param = torch.nn.Parameter(xt)   \n",
    "\n",
    "        \n",
    "            with torch.enable_grad():              \n",
    "                for i in range(K):                \n",
    "                    optimizer = torch.optim.Adagrad([input_embs_param], lr=learning_rate)  \n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    trend, season = ema.ema_model.model(input_embs_param, t)  \n",
    "                    x_start = trend + season\n",
    "\n",
    "                    \n",
    "                    if sigma.mean() == 0:\n",
    "                        logp_term = coef * ((mean - input_embs_param) ** 2 / 1.).mean(dim=0).sum()\n",
    "                        infill_loss = (x_start[partial_mask] - target[partial_mask]) ** 2       \n",
    "                        infill_loss = infill_loss.mean(dim=0).sum()\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    else:\n",
    "                        logp_term = coef * ((mean - input_embs_param)**2 / sigma).mean(dim=0).sum()  \n",
    "                        infill_loss = (x_start[partial_mask] - target[partial_mask]) ** 2       \n",
    "                        infill_loss = (infill_loss/sigma.mean()).mean(dim=0).sum()\n",
    "                        \n",
    "                        \n",
    "        \n",
    "                    loss = logp_term + infill_loss\n",
    "    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epsilon = torch.randn_like(input_embs_param.data)\n",
    "                    input_embs_param = torch.nn.Parameter((input_embs_param.data + coef_ * sigma.mean().item() * epsilon).detach()) \n",
    "\n",
    "\n",
    "            xt[~partial_mask] = input_embs_param.data[~partial_mask]  \n",
    "\n",
    "\n",
    "\n",
    "            noise = torch.randn_like(target)\n",
    "            target_t =  extract(sqrt_alphas_cumprod, t, target.shape) * target +  \\\n",
    "                        extract(sqrt_one_minus_alphas_cumprod, t, target.shape) * noise \n",
    "        \n",
    "           \n",
    "            xt[partial_mask] = target_t[partial_mask]             \n",
    "\n",
    "        \n",
    "        xt[partial_mask] = target[partial_mask]   \n",
    "        \n",
    "        samples = np.row_stack([samples, xt.detach().cpu().numpy()])\n",
    "        reals = np.row_stack([reals, x.detach().cpu().numpy()])\n",
    "        masks = np.row_stack([masks, t_m.detach().cpu().numpy()])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61a0508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0118 0.0071\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import calc_mse,calc_rmse,calc_mae\n",
    "\n",
    "rmse = calc_rmse(samples, reals, 1-masks)\n",
    "mae = calc_mae(samples, reals, 1-masks)  \n",
    "print(round(rmse,4), round(mae,4) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
