{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7cca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from Utils.io_utils import load_yaml_config, seed_everything, merge_opts_to_config, instantiate_from_config\n",
    "from Utils.metric_utils import visualization\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font',family='Times New Roman') \n",
    "\n",
    "import umap                       \n",
    "import seaborn as sns \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a88cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Training Script')\n",
    "    parser.add_argument('--name', type=str, default=None)\n",
    "\n",
    "    parser.add_argument('--config_file', type=str, default=None, \n",
    "                        help='path of config file')\n",
    "    parser.add_argument('--output', type=str, default='OUTPUT', \n",
    "                        help='directory to save the results')\n",
    "    parser.add_argument('--tensorboard', action='store_true', \n",
    "                        help='use tensorboard for logging')\n",
    "\n",
    "    # args for random\n",
    "\n",
    "    parser.add_argument('--cudnn_deterministic', action='store_true', default=False,\n",
    "                        help='set cudnn.deterministic True')\n",
    "    parser.add_argument('--seed', type=int, default=10, \n",
    "                        help='seed for initializing training.')\n",
    "    parser.add_argument('--gpu', type=int, default=None,\n",
    "                        help='GPU id to use. If given, only the specific gpu will be'\n",
    "                        ' used, and ddp will be disabled')\n",
    "    \n",
    "    # args for training\n",
    "    parser.add_argument('--train', action='store_true', default=False, help='Train or Test.')\n",
    "    parser.add_argument('--sample', type=int, default=0, \n",
    "                        choices=[0, 1], help='Condition or Uncondition.')\n",
    "    parser.add_argument('--mode', type=str, default='infill',\n",
    "                        help='Infilling or Forecasting.')\n",
    "    parser.add_argument('--milestone', type=int, default=10)\n",
    "\n",
    "    parser.add_argument('--missing_ratio', type=float, default=0., help='Ratio of Missing Values.')\n",
    "    parser.add_argument('--pred_len', type=int, default=0, help='Length of Predictions.')\n",
    "    \n",
    "    \n",
    "    # args for modify config\n",
    "    parser.add_argument('opts', help='Modify config options using the command-line',\n",
    "                        default=None, nargs=argparse.REMAINDER)  \n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.save_dir = os.path.join(args.output, f'{args.name}')\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a503d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(name=None, config_file='./Config/SP500.yaml', output='OUTPUT', tensorboard=False, cudnn_deterministic=False, seed=10, gpu=0, train=True, sample=0, mode='infill', milestone=10, missing_ratio=0.0, pred_len=0, opts=[], save_dir='OUTPUT\\\\None')\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "\n",
    "data_name = 'SP500'\n",
    "# data_name = 'ETTh1'\n",
    "# data_name = 'ER'\n",
    "# data_name = 'Energy'\n",
    "# data_name = 'weather'\n",
    "\n",
    "\n",
    "if data_name == 'SP500':\n",
    "    args.config_file =  './Config/SP500.yaml'\n",
    "\n",
    "elif data_name == 'ER':\n",
    "    args.config_file =  './Config/exchange_rate.yaml'\n",
    "    \n",
    "elif data_name == 'ETTh1':\n",
    "    args.config_file =  './Config/ETTh1.yaml'\n",
    "\n",
    "elif data_name == 'Energy':\n",
    "    args.config_file =  './Config/Energy.yaml'\n",
    "    \n",
    "elif data_name == 'weather':\n",
    "    args.config_file =  './Config/weather.yaml'\n",
    "    \n",
    "elif data_name == 'MuJoCo':\n",
    "    args.config_file =  './Config/MuJoCo.yaml'\n",
    "    \n",
    "args.gpu =  0\n",
    "args.train = True\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3918163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 10\n",
      "{'model': {'target': 'Models.interpretable_diffusion.gaussian_diffusion.Diffusion_TS', 'params': {'seq_length': 30, 'feature_size': 6, 'n_layer_enc': 2, 'n_layer_dec': 2, 'd_model': 64, 'timesteps': 500, 'sampling_timesteps': 500, 'loss_type': 'l1', 'beta_schedule': 'cosine', 'n_heads': 4, 'mlp_hidden_times': 4, 'attn_pd': 0.0, 'resid_pd': 0.0, 'kernel_size': 1, 'padding_size': 0}}, 'solver': {'base_lr': 1e-05, 'max_epochs': 10000, 'results_folder': './Checkpoints_stock', 'gradient_accumulate_every': 2, 'save_cycle': 500, 'ema': {'decay': 0.995, 'update_interval': 10}, 'scheduler': {'target': 'engine.lr_sch.ReduceLROnPlateauWithWarmup', 'params': {'factor': 0.5, 'patience': 2000, 'min_lr': 1e-05, 'threshold': 0.1, 'threshold_mode': 'rel', 'warmup_lr': 0.0008, 'warmup': 500, 'verbose': False}}}, 'dataloader': {'train_dataset': {'target': 'Utils.Data_utils.real_datasets.CustomDataset', 'params': {'name': 'stock', 'proportion': 1.0, 'data_root': './Data/datasets/stock_data.csv', 'window': 24, 'save2npy': True, 'neg_one_to_one': True, 'seed': 123, 'period': 'train'}}, 'test_dataset': {'target': 'Utils.Data_utils.real_datasets.CustomDataset', 'params': {'name': 'stock', 'proportion': 0.9, 'data_root': './Data/datasets/stock_data.csv', 'window': 24, 'save2npy': True, 'neg_one_to_one': True, 'seed': 123, 'period': 'test', 'style': 'separate', 'distribution': 'geometric'}, 'coefficient': 0.01, 'step_size': 0.05, 'sampling_steps': 200}, 'batch_size': 128, 'sample_size': 256, 'shuffle': True}}\n",
      "cosine 500\n"
     ]
    }
   ],
   "source": [
    "if args.seed is not None:\n",
    "    seed_everything(args.seed)\n",
    "\n",
    "if args.gpu is not None:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "config = load_yaml_config(args.config_file)\n",
    "config = merge_opts_to_config(config, args.opts)\n",
    "\n",
    "\n",
    "config['dataloader']['batch_size'] = 128\n",
    "print(config)\n",
    "\n",
    "beta_schedule = config['model']['params']['beta_schedule'] \n",
    "timesteps = config['model']['params']['timesteps'] \n",
    "print(beta_schedule, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3406e18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30, 6])\n",
      "5775\n",
      "[0.19777922 0.19484571 0.0380807  0.19849203 0.19913802 0.18855503]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if data_name == 'SP500':\n",
    "    load_data = np.load( './Data/datasets/sp500.npy')\n",
    "\n",
    "elif data_name == 'ETTh1':\n",
    "    load_data = np.load( './Data/datasets/ETTh1.npy')\n",
    "    \n",
    "elif data_name == 'ER':\n",
    "    load_data = np.load( './Data/datasets/exchange_rate.npy')\n",
    "    \n",
    "elif data_name == 'Energy':\n",
    "    load_data = np.load( './Data/datasets/Energy.npy')\n",
    "    \n",
    "elif data_name == 'weather':\n",
    "    load_data = np.load( './Data/datasets/weather.npy')\n",
    "\n",
    "elif data_name == 'MuJoCo':\n",
    "    load_data = np.load( './Data/datasets/MuJoCo.npy')\n",
    "    \n",
    "    \n",
    "np.random.shuffle(load_data)\n",
    "train_data = load_data\n",
    "\n",
    "\n",
    "Numble = train_data.shape[0]\n",
    "Length = train_data.shape[1]\n",
    "Feature = train_data.shape[2]\n",
    "Batchsize = 128\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X_data[idx], dtype=torch.float32)  \n",
    "        return x   \n",
    "                    \n",
    "dataset = MyDataset(train_data)    \n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "def cycle(dl):         \n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "            \n",
    "dl = cycle( dataloader )\n",
    "print( next(dl).shape )\n",
    "print(Numble)\n",
    "\n",
    "print( load_data[0][0]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f959d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54772621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "\n",
    "\n",
    "def introduce_missing_superior_to_mean(X):\n",
    "    print(\"Introducing missing data with > mean\")\n",
    "    N, D = X.shape\n",
    "    Xnan = X.copy()\n",
    "\n",
    "    # ---- MNAR in D/2 dimensions\n",
    "    mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
    "    ix_larger_than_mean = Xnan[:, :int(D / 2)] > mean\n",
    "    Xnan[:, :int(D / 2)][ix_larger_than_mean] = np.nan    # 大于均值的为nan\n",
    "\n",
    "    Xnan = Xnan.astype(np.float32)\n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan.astype(np.float32))] = 0   # nan处级缺失处设置为0\n",
    "\n",
    "    return Xnan, Xz\n",
    "\n",
    "\n",
    "def introduce_missing_mean_values(X, percentage_to_remove = 30):\n",
    "    print(\"Introduce missing data by removing the values around the mean\")\n",
    "    N, D = X.shape\n",
    "    Xnan = X.copy()\n",
    "\n",
    "    num_elements = int(N * percentage_to_remove / 100)   # number of elements to remove\n",
    "\n",
    "    # ---- MNAR in D/2 dimensions\n",
    "    mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
    "    abs_diff_from_mean = np.abs(Xnan[:, :int(D / 2)] - mean)  # 每个值和均值的差\n",
    "    indices_to_remove = np.argsort(abs_diff_from_mean, axis = 0)[:num_elements]  # np.argsort 按升序排序后的索引  \n",
    "    # Set those values to NaN                                                    # 这里相当于取了和均值的差值在前30%的索引 \n",
    "    for d in range(indices_to_remove.shape[1]):\n",
    "        Xnan[indices_to_remove[:, d], d] = np.nan\n",
    "    Xnan = Xnan.astype(np.float32)\n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan)] = 0\n",
    "    return Xnan, Xz\n",
    "\n",
    "\n",
    "def introduce_missing_extreme_values(X, percentile_extreme = 25):\n",
    "    print(\"Introducing missing data via removing extreme values\")\n",
    "    N, D = X.shape\n",
    "    Xnan = X.copy()\n",
    "\n",
    "    # ---- MNAR in D/2 dimensions\n",
    "    lower_bound = np.percentile(Xnan[:, :int(D / 2)], percentile_extreme, axis=0)\n",
    "    upper_bound = np.percentile(Xnan[:, :int(D / 2)], 100 - percentile_extreme, axis=0)\n",
    "\n",
    "    ix_lower = Xnan[:, :int(D / 2)] < lower_bound        # 小于下界的值的索引\n",
    "    ix_higher = Xnan[:, :int(D / 2)] > upper_bound\n",
    "    Xnan[:, :int(D / 2)][ix_lower | ix_higher] = np.nan  # 过大过小的去掉 换成nan\n",
    "    Xnan = Xnan.astype(np.float32)\n",
    "    \n",
    "    Xz = Xnan.copy()\n",
    "    Xz[np.isnan(Xnan)] = 0    # 换成0\n",
    "\n",
    "    return Xnan, Xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a4e908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduce missing data by removing the values around the mean\n",
      "(5775, 30, 6)\n",
      "0.15\n"
     ]
    }
   ],
   "source": [
    "X_data = load_data.reshape(-1, Numble*Length,Feature).squeeze(0)\n",
    "\n",
    "Xnan, Xz = introduce_missing_mean_values( X_data)       # 有nan\n",
    "# Xnan, Xz = introduce_missing_superior_to_mean( X_data)\n",
    "# Xnan, Xz = introduce_missing_extreme_values( X_data)\n",
    "\n",
    "\n",
    "Xnan = Xnan.reshape(-1, Length, Feature)\n",
    "S = np.array(~np.isnan(Xnan), dtype=np.float32)\n",
    "print(S.shape)\n",
    "\n",
    "\n",
    "n_1 = np.sum(S == 1)\n",
    "n_0 = np.sum(S == 0)\n",
    "print(n_0/(n_1+n_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69184c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "number = load_data.shape[0]\n",
    "L = load_data.shape[1]\n",
    "D = load_data.shape[2]\n",
    "\n",
    "\n",
    "'去掉全部缺失的列'\n",
    "for i in range( number ):\n",
    "    for j in range(D):\n",
    "        if np.isnan(Xnan[i][:,j]).all() == True:\n",
    "            Xnan[i][:,j] = load_data[i][:,j]\n",
    "            S[i][:,j] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "236f5247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0667965367965368\n"
     ]
    }
   ],
   "source": [
    "'去掉完全缺失属性后的缺失率'\n",
    "n_1 = np.sum(S == 1)\n",
    "n_0 = np.sum(S == 0)\n",
    "print(n_0/(n_1+n_0))\n",
    "\n",
    "train_set =  {\"X\":  Xnan}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af700df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
      "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
      "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
      "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
      "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
      "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
      "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pypots.imputation import Pyraformer\n",
    "from pypots.imputation import SAITS  \n",
    "from pypots.imputation import ImputeFormer\n",
    "from pypots.imputation import CSDI\n",
    "# from pypots.optim import Adam\n",
    "# from pypots.imputation import TimesNet\n",
    "# from pypots.imputation import TimeMixer\n",
    "# from pypots.imputation import TimeMixerPP\n",
    "# from pypots.imputation import FEDformer\n",
    "from pypots.imputation import Informer\n",
    "# from pypots.imputation import ETSformer\n",
    "# from pypots.imputation import Reformer\n",
    "# from pypots.imputation import MICN\n",
    "# from pypots.imputation import PatchTST\n",
    "# from pypots.imputation import TiDE\n",
    "# from pypots.imputation import Koopa\n",
    "# from pypots.imputation import DLinear\n",
    "\n",
    "\n",
    "baseline = 'CSDI'\n",
    "# baseline = 'SAITS'\n",
    "# baseline = 'Pyraformer'\n",
    "# baseline = 'ImputeFormer'\n",
    "# baseline = 'Informer'\n",
    "# baseline = 'TimesNet'\n",
    "# baseline = 'PatchTST'\n",
    "# baseline = 'ETSformer'\n",
    "# baseline = 'TimeMixer'\n",
    "# baseline = 'DLinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a53585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 23:21:01 [INFO]: No given device, using default device: cuda\n",
      "2026-01-31 23:21:01 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2026-01-31 23:21:01 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,693,761\n",
      "2026-01-31 23:21:13 [INFO]: Epoch 001 - training loss (default): 0.3138\n",
      "2026-01-31 23:21:19 [INFO]: Epoch 002 - training loss (default): 0.0700\n",
      "2026-01-31 23:21:25 [INFO]: Epoch 003 - training loss (default): 0.0430\n",
      "2026-01-31 23:21:32 [INFO]: Epoch 004 - training loss (default): 0.0395\n",
      "2026-01-31 23:21:38 [INFO]: Epoch 005 - training loss (default): 0.0293\n",
      "2026-01-31 23:21:45 [INFO]: Epoch 006 - training loss (default): 0.0288\n",
      "2026-01-31 23:21:51 [INFO]: Epoch 007 - training loss (default): 0.0234\n",
      "2026-01-31 23:21:57 [INFO]: Epoch 008 - training loss (default): 0.0230\n",
      "2026-01-31 23:22:04 [INFO]: Epoch 009 - training loss (default): 0.0222\n",
      "2026-01-31 23:22:10 [INFO]: Epoch 010 - training loss (default): 0.0223\n",
      "2026-01-31 23:22:17 [INFO]: Epoch 011 - training loss (default): 0.0228\n",
      "2026-01-31 23:22:23 [INFO]: Epoch 012 - training loss (default): 0.0205\n",
      "2026-01-31 23:22:30 [INFO]: Epoch 013 - training loss (default): 0.0199\n",
      "2026-01-31 23:22:36 [INFO]: Epoch 014 - training loss (default): 0.0198\n",
      "2026-01-31 23:22:43 [INFO]: Epoch 015 - training loss (default): 0.0191\n",
      "2026-01-31 23:22:49 [INFO]: Epoch 016 - training loss (default): 0.0179\n",
      "2026-01-31 23:22:56 [INFO]: Epoch 017 - training loss (default): 0.0154\n",
      "2026-01-31 23:23:02 [INFO]: Epoch 018 - training loss (default): 0.0176\n",
      "2026-01-31 23:23:09 [INFO]: Epoch 019 - training loss (default): 0.0174\n",
      "2026-01-31 23:23:15 [INFO]: Epoch 020 - training loss (default): 0.0168\n",
      "2026-01-31 23:23:22 [INFO]: Epoch 021 - training loss (default): 0.0146\n",
      "2026-01-31 23:23:29 [INFO]: Epoch 022 - training loss (default): 0.0179\n",
      "2026-01-31 23:23:35 [INFO]: Epoch 023 - training loss (default): 0.0155\n",
      "2026-01-31 23:23:42 [INFO]: Epoch 024 - training loss (default): 0.0144\n",
      "2026-01-31 23:23:48 [INFO]: Epoch 025 - training loss (default): 0.0155\n",
      "2026-01-31 23:23:55 [INFO]: Epoch 026 - training loss (default): 0.0131\n",
      "2026-01-31 23:24:02 [INFO]: Epoch 027 - training loss (default): 0.0151\n",
      "2026-01-31 23:24:08 [INFO]: Epoch 028 - training loss (default): 0.0130\n",
      "2026-01-31 23:24:15 [INFO]: Epoch 029 - training loss (default): 0.0155\n",
      "2026-01-31 23:24:21 [INFO]: Epoch 030 - training loss (default): 0.0123\n",
      "2026-01-31 23:24:28 [INFO]: Epoch 031 - training loss (default): 0.0140\n",
      "2026-01-31 23:24:34 [INFO]: Epoch 032 - training loss (default): 0.0138\n",
      "2026-01-31 23:24:41 [INFO]: Epoch 033 - training loss (default): 0.0138\n",
      "2026-01-31 23:24:48 [INFO]: Epoch 034 - training loss (default): 0.0138\n",
      "2026-01-31 23:24:54 [INFO]: Epoch 035 - training loss (default): 0.0159\n",
      "2026-01-31 23:25:01 [INFO]: Epoch 036 - training loss (default): 0.0144\n",
      "2026-01-31 23:25:07 [INFO]: Epoch 037 - training loss (default): 0.0138\n",
      "2026-01-31 23:25:14 [INFO]: Epoch 038 - training loss (default): 0.0113\n",
      "2026-01-31 23:25:20 [INFO]: Epoch 039 - training loss (default): 0.0097\n",
      "2026-01-31 23:25:27 [INFO]: Epoch 040 - training loss (default): 0.0113\n",
      "2026-01-31 23:25:34 [INFO]: Epoch 041 - training loss (default): 0.0128\n",
      "2026-01-31 23:25:40 [INFO]: Epoch 042 - training loss (default): 0.0130\n",
      "2026-01-31 23:25:47 [INFO]: Epoch 043 - training loss (default): 0.0119\n",
      "2026-01-31 23:25:53 [INFO]: Epoch 044 - training loss (default): 0.0132\n",
      "2026-01-31 23:26:00 [INFO]: Epoch 045 - training loss (default): 0.0113\n",
      "2026-01-31 23:26:06 [INFO]: Epoch 046 - training loss (default): 0.0124\n",
      "2026-01-31 23:26:13 [INFO]: Epoch 047 - training loss (default): 0.0112\n",
      "2026-01-31 23:26:19 [INFO]: Epoch 048 - training loss (default): 0.0116\n",
      "2026-01-31 23:26:26 [INFO]: Epoch 049 - training loss (default): 0.0129\n",
      "2026-01-31 23:26:33 [INFO]: Epoch 050 - training loss (default): 0.0118\n",
      "2026-01-31 23:26:39 [INFO]: Epoch 051 - training loss (default): 0.0121\n",
      "2026-01-31 23:26:46 [INFO]: Epoch 052 - training loss (default): 0.0112\n",
      "2026-01-31 23:26:52 [INFO]: Epoch 053 - training loss (default): 0.0103\n",
      "2026-01-31 23:26:59 [INFO]: Epoch 054 - training loss (default): 0.0116\n",
      "2026-01-31 23:27:05 [INFO]: Epoch 055 - training loss (default): 0.0121\n",
      "2026-01-31 23:27:12 [INFO]: Epoch 056 - training loss (default): 0.0118\n",
      "2026-01-31 23:27:19 [INFO]: Epoch 057 - training loss (default): 0.0114\n",
      "2026-01-31 23:27:25 [INFO]: Epoch 058 - training loss (default): 0.0120\n",
      "2026-01-31 23:27:32 [INFO]: Epoch 059 - training loss (default): 0.0125\n",
      "2026-01-31 23:27:38 [INFO]: Epoch 060 - training loss (default): 0.0125\n",
      "2026-01-31 23:27:45 [INFO]: Epoch 061 - training loss (default): 0.0103\n",
      "2026-01-31 23:27:51 [INFO]: Epoch 062 - training loss (default): 0.0109\n",
      "2026-01-31 23:27:58 [INFO]: Epoch 063 - training loss (default): 0.0116\n",
      "2026-01-31 23:28:04 [INFO]: Epoch 064 - training loss (default): 0.0105\n",
      "2026-01-31 23:28:11 [INFO]: Epoch 065 - training loss (default): 0.0107\n",
      "2026-01-31 23:28:18 [INFO]: Epoch 066 - training loss (default): 0.0117\n",
      "2026-01-31 23:28:24 [INFO]: Epoch 067 - training loss (default): 0.0118\n",
      "2026-01-31 23:28:31 [INFO]: Epoch 068 - training loss (default): 0.0104\n",
      "2026-01-31 23:28:37 [INFO]: Epoch 069 - training loss (default): 0.0111\n",
      "2026-01-31 23:28:44 [INFO]: Epoch 070 - training loss (default): 0.0123\n",
      "2026-01-31 23:28:51 [INFO]: Epoch 071 - training loss (default): 0.0097\n",
      "2026-01-31 23:28:57 [INFO]: Epoch 072 - training loss (default): 0.0115\n",
      "2026-01-31 23:29:04 [INFO]: Epoch 073 - training loss (default): 0.0109\n",
      "2026-01-31 23:29:10 [INFO]: Epoch 074 - training loss (default): 0.0131\n",
      "2026-01-31 23:29:17 [INFO]: Epoch 075 - training loss (default): 0.0117\n",
      "2026-01-31 23:29:24 [INFO]: Epoch 076 - training loss (default): 0.0109\n",
      "2026-01-31 23:29:30 [INFO]: Epoch 077 - training loss (default): 0.0105\n",
      "2026-01-31 23:29:37 [INFO]: Epoch 078 - training loss (default): 0.0105\n",
      "2026-01-31 23:29:43 [INFO]: Epoch 079 - training loss (default): 0.0114\n",
      "2026-01-31 23:29:50 [INFO]: Epoch 080 - training loss (default): 0.0120\n",
      "2026-01-31 23:29:56 [INFO]: Epoch 081 - training loss (default): 0.0123\n",
      "2026-01-31 23:30:03 [INFO]: Epoch 082 - training loss (default): 0.0100\n",
      "2026-01-31 23:30:10 [INFO]: Epoch 083 - training loss (default): 0.0095\n",
      "2026-01-31 23:30:16 [INFO]: Epoch 084 - training loss (default): 0.0103\n",
      "2026-01-31 23:30:23 [INFO]: Epoch 085 - training loss (default): 0.0115\n",
      "2026-01-31 23:30:29 [INFO]: Epoch 086 - training loss (default): 0.0116\n",
      "2026-01-31 23:30:36 [INFO]: Epoch 087 - training loss (default): 0.0114\n",
      "2026-01-31 23:30:42 [INFO]: Epoch 088 - training loss (default): 0.0106\n",
      "2026-01-31 23:30:49 [INFO]: Epoch 089 - training loss (default): 0.0100\n",
      "2026-01-31 23:30:56 [INFO]: Epoch 090 - training loss (default): 0.0110\n",
      "2026-01-31 23:31:02 [INFO]: Epoch 091 - training loss (default): 0.0119\n",
      "2026-01-31 23:31:09 [INFO]: Epoch 092 - training loss (default): 0.0102\n",
      "2026-01-31 23:31:15 [INFO]: Epoch 093 - training loss (default): 0.0105\n",
      "2026-01-31 23:31:22 [INFO]: Epoch 094 - training loss (default): 0.0113\n",
      "2026-01-31 23:31:28 [INFO]: Epoch 095 - training loss (default): 0.0111\n",
      "2026-01-31 23:31:35 [INFO]: Epoch 096 - training loss (default): 0.0106\n",
      "2026-01-31 23:31:41 [INFO]: Epoch 097 - training loss (default): 0.0100\n",
      "2026-01-31 23:31:48 [INFO]: Epoch 098 - training loss (default): 0.0095\n",
      "2026-01-31 23:31:55 [INFO]: Epoch 099 - training loss (default): 0.0109\n",
      "2026-01-31 23:32:01 [INFO]: Epoch 100 - training loss (default): 0.0094\n",
      "2026-01-31 23:32:08 [INFO]: Epoch 101 - training loss (default): 0.0094\n",
      "2026-01-31 23:32:14 [INFO]: Epoch 102 - training loss (default): 0.0101\n",
      "2026-01-31 23:32:21 [INFO]: Epoch 103 - training loss (default): 0.0102\n",
      "2026-01-31 23:32:27 [INFO]: Epoch 104 - training loss (default): 0.0097\n",
      "2026-01-31 23:32:34 [INFO]: Epoch 105 - training loss (default): 0.0098\n",
      "2026-01-31 23:32:41 [INFO]: Epoch 106 - training loss (default): 0.0097\n",
      "2026-01-31 23:32:47 [INFO]: Epoch 107 - training loss (default): 0.0090\n",
      "2026-01-31 23:32:54 [INFO]: Epoch 108 - training loss (default): 0.0092\n",
      "2026-01-31 23:33:00 [INFO]: Epoch 109 - training loss (default): 0.0109\n",
      "2026-01-31 23:33:07 [INFO]: Epoch 110 - training loss (default): 0.0102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 23:33:13 [INFO]: Epoch 111 - training loss (default): 0.0093\n",
      "2026-01-31 23:33:20 [INFO]: Epoch 112 - training loss (default): 0.0110\n",
      "2026-01-31 23:33:27 [INFO]: Epoch 113 - training loss (default): 0.0099\n",
      "2026-01-31 23:33:33 [INFO]: Epoch 114 - training loss (default): 0.0084\n",
      "2026-01-31 23:33:40 [INFO]: Epoch 115 - training loss (default): 0.0093\n",
      "2026-01-31 23:33:46 [INFO]: Epoch 116 - training loss (default): 0.0107\n",
      "2026-01-31 23:33:53 [INFO]: Epoch 117 - training loss (default): 0.0098\n",
      "2026-01-31 23:34:00 [INFO]: Epoch 118 - training loss (default): 0.0102\n",
      "2026-01-31 23:34:06 [INFO]: Epoch 119 - training loss (default): 0.0093\n",
      "2026-01-31 23:34:13 [INFO]: Epoch 120 - training loss (default): 0.0089\n",
      "2026-01-31 23:34:19 [INFO]: Epoch 121 - training loss (default): 0.0099\n",
      "2026-01-31 23:34:26 [INFO]: Epoch 122 - training loss (default): 0.0091\n",
      "2026-01-31 23:34:33 [INFO]: Epoch 123 - training loss (default): 0.0100\n",
      "2026-01-31 23:34:39 [INFO]: Epoch 124 - training loss (default): 0.0107\n",
      "2026-01-31 23:34:46 [INFO]: Epoch 125 - training loss (default): 0.0095\n",
      "2026-01-31 23:34:53 [INFO]: Epoch 126 - training loss (default): 0.0114\n",
      "2026-01-31 23:34:59 [INFO]: Epoch 127 - training loss (default): 0.0103\n",
      "2026-01-31 23:35:06 [INFO]: Epoch 128 - training loss (default): 0.0098\n",
      "2026-01-31 23:35:12 [INFO]: Epoch 129 - training loss (default): 0.0109\n",
      "2026-01-31 23:35:19 [INFO]: Epoch 130 - training loss (default): 0.0103\n",
      "2026-01-31 23:35:25 [INFO]: Epoch 131 - training loss (default): 0.0117\n",
      "2026-01-31 23:35:32 [INFO]: Epoch 132 - training loss (default): 0.0102\n",
      "2026-01-31 23:35:39 [INFO]: Epoch 133 - training loss (default): 0.0083\n",
      "2026-01-31 23:35:45 [INFO]: Epoch 134 - training loss (default): 0.0099\n",
      "2026-01-31 23:35:52 [INFO]: Epoch 135 - training loss (default): 0.0111\n",
      "2026-01-31 23:35:58 [INFO]: Epoch 136 - training loss (default): 0.0098\n",
      "2026-01-31 23:36:05 [INFO]: Epoch 137 - training loss (default): 0.0094\n",
      "2026-01-31 23:36:11 [INFO]: Epoch 138 - training loss (default): 0.0086\n",
      "2026-01-31 23:36:18 [INFO]: Epoch 139 - training loss (default): 0.0106\n",
      "2026-01-31 23:36:24 [INFO]: Epoch 140 - training loss (default): 0.0087\n",
      "2026-01-31 23:36:31 [INFO]: Epoch 141 - training loss (default): 0.0108\n",
      "2026-01-31 23:36:38 [INFO]: Epoch 142 - training loss (default): 0.0093\n",
      "2026-01-31 23:36:44 [INFO]: Epoch 143 - training loss (default): 0.0110\n",
      "2026-01-31 23:36:51 [INFO]: Epoch 144 - training loss (default): 0.0092\n",
      "2026-01-31 23:36:57 [INFO]: Epoch 145 - training loss (default): 0.0097\n",
      "2026-01-31 23:37:04 [INFO]: Epoch 146 - training loss (default): 0.0105\n",
      "2026-01-31 23:37:11 [INFO]: Epoch 147 - training loss (default): 0.0088\n",
      "2026-01-31 23:37:17 [INFO]: Epoch 148 - training loss (default): 0.0088\n",
      "2026-01-31 23:37:24 [INFO]: Epoch 149 - training loss (default): 0.0089\n",
      "2026-01-31 23:37:30 [INFO]: Epoch 150 - training loss (default): 0.0097\n",
      "2026-01-31 23:37:37 [INFO]: Epoch 151 - training loss (default): 0.0099\n",
      "2026-01-31 23:37:43 [INFO]: Epoch 152 - training loss (default): 0.0089\n",
      "2026-01-31 23:37:50 [INFO]: Epoch 153 - training loss (default): 0.0103\n",
      "2026-01-31 23:37:57 [INFO]: Epoch 154 - training loss (default): 0.0091\n",
      "2026-01-31 23:38:03 [INFO]: Epoch 155 - training loss (default): 0.0096\n",
      "2026-01-31 23:38:10 [INFO]: Epoch 156 - training loss (default): 0.0100\n",
      "2026-01-31 23:38:17 [INFO]: Epoch 157 - training loss (default): 0.0116\n",
      "2026-01-31 23:38:23 [INFO]: Epoch 158 - training loss (default): 0.0096\n",
      "2026-01-31 23:38:30 [INFO]: Epoch 159 - training loss (default): 0.0097\n",
      "2026-01-31 23:38:36 [INFO]: Epoch 160 - training loss (default): 0.0096\n",
      "2026-01-31 23:38:43 [INFO]: Epoch 161 - training loss (default): 0.0098\n",
      "2026-01-31 23:38:49 [INFO]: Epoch 162 - training loss (default): 0.0087\n",
      "2026-01-31 23:38:56 [INFO]: Epoch 163 - training loss (default): 0.0079\n",
      "2026-01-31 23:39:03 [INFO]: Epoch 164 - training loss (default): 0.0076\n",
      "2026-01-31 23:39:09 [INFO]: Epoch 165 - training loss (default): 0.0087\n",
      "2026-01-31 23:39:16 [INFO]: Epoch 166 - training loss (default): 0.0089\n",
      "2026-01-31 23:39:22 [INFO]: Epoch 167 - training loss (default): 0.0083\n",
      "2026-01-31 23:39:26 [INFO]: Epoch 168 - training loss (default): 0.0088\n",
      "2026-01-31 23:39:30 [INFO]: Epoch 169 - training loss (default): 0.0088\n",
      "2026-01-31 23:39:34 [INFO]: Epoch 170 - training loss (default): 0.0077\n",
      "2026-01-31 23:39:38 [INFO]: Epoch 171 - training loss (default): 0.0100\n",
      "2026-01-31 23:39:42 [INFO]: Epoch 172 - training loss (default): 0.0094\n",
      "2026-01-31 23:39:46 [INFO]: Epoch 173 - training loss (default): 0.0093\n",
      "2026-01-31 23:39:50 [INFO]: Epoch 174 - training loss (default): 0.0101\n",
      "2026-01-31 23:39:54 [INFO]: Epoch 175 - training loss (default): 0.0104\n",
      "2026-01-31 23:39:58 [INFO]: Epoch 176 - training loss (default): 0.0095\n",
      "2026-01-31 23:40:02 [INFO]: Epoch 177 - training loss (default): 0.0086\n",
      "2026-01-31 23:40:06 [INFO]: Epoch 178 - training loss (default): 0.0093\n",
      "2026-01-31 23:40:10 [INFO]: Epoch 179 - training loss (default): 0.0090\n",
      "2026-01-31 23:40:14 [INFO]: Epoch 180 - training loss (default): 0.0089\n",
      "2026-01-31 23:40:18 [INFO]: Epoch 181 - training loss (default): 0.0085\n",
      "2026-01-31 23:40:22 [INFO]: Epoch 182 - training loss (default): 0.0092\n",
      "2026-01-31 23:40:26 [INFO]: Epoch 183 - training loss (default): 0.0081\n",
      "2026-01-31 23:40:30 [INFO]: Epoch 184 - training loss (default): 0.0084\n",
      "2026-01-31 23:40:34 [INFO]: Epoch 185 - training loss (default): 0.0096\n",
      "2026-01-31 23:40:38 [INFO]: Epoch 186 - training loss (default): 0.0089\n",
      "2026-01-31 23:40:42 [INFO]: Epoch 187 - training loss (default): 0.0108\n",
      "2026-01-31 23:40:46 [INFO]: Epoch 188 - training loss (default): 0.0098\n",
      "2026-01-31 23:40:50 [INFO]: Epoch 189 - training loss (default): 0.0107\n",
      "2026-01-31 23:40:54 [INFO]: Epoch 190 - training loss (default): 0.0093\n",
      "2026-01-31 23:40:58 [INFO]: Epoch 191 - training loss (default): 0.0088\n",
      "2026-01-31 23:41:02 [INFO]: Epoch 192 - training loss (default): 0.0104\n",
      "2026-01-31 23:41:06 [INFO]: Epoch 193 - training loss (default): 0.0098\n",
      "2026-01-31 23:41:10 [INFO]: Epoch 194 - training loss (default): 0.0094\n",
      "2026-01-31 23:41:14 [INFO]: Epoch 195 - training loss (default): 0.0085\n",
      "2026-01-31 23:41:18 [INFO]: Epoch 196 - training loss (default): 0.0101\n",
      "2026-01-31 23:41:22 [INFO]: Epoch 197 - training loss (default): 0.0090\n",
      "2026-01-31 23:41:26 [INFO]: Epoch 198 - training loss (default): 0.0110\n",
      "2026-01-31 23:41:30 [INFO]: Epoch 199 - training loss (default): 0.0091\n",
      "2026-01-31 23:41:34 [INFO]: Epoch 200 - training loss (default): 0.0085\n",
      "2026-01-31 23:41:34 [INFO]: Finished training. The best model is from epoch#164.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "if baseline == 'SAITS':\n",
    "    saits = SAITS(n_steps=Xnan.shape[1], n_features=Xnan.shape[2],\n",
    "                  n_layers=2, d_model=256, n_heads=4, d_k=64, d_v=64, d_ffn=128, dropout=0.1, \n",
    "    #               n_layers=2, d_model=256, n_heads=4, d_k=128, d_v=128, d_ffn=128, dropout=0.1, \n",
    "                  ORT_weight = 1, MIT_weight = 1,\n",
    "                  num_workers=0, \n",
    "                  batch_size=200, epochs=300, )   \n",
    "\n",
    "    saits.fit( train_set = train_set ) \n",
    "\n",
    "    \n",
    "# d_proj= 36, d_ffn=256 时模型不收敛\n",
    "# 其他情况和数据集有关 考虑修改一些参数来让模型收敛\n",
    "elif baseline == 'ImputeFormer':\n",
    "    imputeFormer = ImputeFormer( \n",
    "        n_steps = Xnan.shape[1], n_features = Xnan.shape[2],\n",
    "        n_layers=4,\n",
    "        d_input_embed= 24, d_learnable_embed=128,     \n",
    "        d_proj= 36,  d_ffn=128, \n",
    "        n_temporal_heads=4, \n",
    "        ORT_weight=1, MIT_weight=1,\n",
    "        batch_size=200, epochs=300,  )\n",
    "\n",
    "    imputeFormer.fit( train_set = train_set  )\n",
    "\n",
    "\n",
    "elif baseline == 'CSDI':\n",
    "    csdi = CSDI(\n",
    "        n_steps= Xnan.shape[1], n_features= Xnan.shape[2],\n",
    "        n_layers=6, n_heads=2, n_channels=128,\n",
    "        d_time_embedding=64, d_feature_embedding=32, d_diffusion_embedding=128,\n",
    "        target_strategy=\"random\",\n",
    "        n_diffusion_steps=50,\n",
    "        batch_size=200, epochs=200,   # epoch200基本足够\n",
    "#         optimizer=Adam(lr=1e-3), # 和使用默认的优化器效果差不多\n",
    "        num_workers=0, )\n",
    "\n",
    "    csdi.fit( train_set = train_set )\n",
    "\n",
    "\n",
    "elif baseline == 'Pyraformer':\n",
    "    pyraformer = Pyraformer( \n",
    "        n_steps = train_data.shape[1], n_features = train_data.shape[2],\n",
    "        n_layers= 4, n_heads = 4,\n",
    "        d_model = 256, d_ffn = 512,\n",
    "        window_size = [1, 1, 1],\n",
    "        inner_size = 3,\n",
    "        dropout = 0.1,\n",
    "        batch_size=200, epochs=500,  )\n",
    "\n",
    "    pyraformer.fit( train_set = train_set  )\n",
    "\n",
    "\n",
    "elif baseline == 'Informer':\n",
    "    informer = Informer( \n",
    "    n_steps = train_data.shape[1], n_features = train_data.shape[2],\n",
    "    n_layers=4, n_heads=8,\n",
    "    d_model=256, d_ffn=512,\n",
    "    factor=5,\n",
    "    batch_size=200, epochs=400,  )\n",
    "\n",
    "    informer.fit( train_set = train_set  )\n",
    "\n",
    "    \n",
    "# 参数 https://github.com/salesforce/ETSformer/blob/main/run.py\n",
    "elif baseline == 'ETSformer':\n",
    "    eTSformer = ETSformer( \n",
    "        n_steps = train_data.shape[1], n_features = train_data.shape[2],\n",
    "        n_heads=8,\n",
    "        d_model=256, d_ffn=512,\n",
    "        top_k=1,\n",
    "        n_encoder_layers=2, n_decoder_layers=2,\n",
    "        batch_size=200, epochs=500,  )\n",
    "\n",
    "    eTSformer.fit( train_set = train_set  )\n",
    "    \n",
    "\n",
    "    \n",
    "elif baseline == 'PatchTST':\n",
    "    patchTST = PatchTST( \n",
    "        n_steps = train_data.shape[1], n_features = train_data.shape[2],\n",
    "        patch_size = 16,  patch_stride = 8,\n",
    "        n_layers = 4,  d_model=256, n_heads=4,\n",
    "        d_k = 64, d_v = 64, d_ffn = 128 ,\n",
    "        dropout = 0.2, attn_dropout=0,\n",
    "        batch_size=200, epochs=500,  )\n",
    "\n",
    "    eTSformer.fit( train_set = train_set  )\n",
    "    \n",
    "    \n",
    "elif baseline == 'TimesNet':\n",
    "    timesNet = TimesNet(\n",
    "        n_steps = train_data.shape[1], n_features = train_data.shape[2],\n",
    "        n_layers = 4, top_k = 1,\n",
    "        d_model = 256, d_ffn = 256,\n",
    "        n_kernels = 5, dropout = 0,\n",
    "        apply_nonstationary_norm = False,\n",
    "        batch_size = 200,  epochs = 400,  )\n",
    "    \n",
    "    timesNet.fit( train_set = train_set  )\n",
    "    \n",
    "    \n",
    "elif baseline == 'TimeMixer':\n",
    "    timeMixer = TimeMixer(\n",
    "        n_steps = train_data.shape[1], n_features = train_data.shape[2],\n",
    "        n_layers = 4, top_k = 1,\n",
    "        d_model = 256, d_ffn = 256,\n",
    "        batch_size = 200,  epochs = 400,  )\n",
    "    \n",
    "    timeMixer.fit( train_set = train_set  )\n",
    "    \n",
    "    \n",
    "# 参数  https://blog.csdn.net/java1314777/article/details/134670578\n",
    "# https://github.com/Thinklab-SJTU/Crossformer\n",
    "elif baseline == 'Crossformer':\n",
    "    crossformer = Crossformer( \n",
    "        n_steps = train_data.shape[1], n_features = train_data.shape[2],\n",
    "        n_layers=4, n_heads=4,\n",
    "        d_model=256, d_ffn=512,\n",
    "        factor=5,\n",
    "        seg_len=6, win_size=2,\n",
    "        batch_size=200, epochs=200,  )\n",
    "    \n",
    "    crossformer.fit( train_set = train_set  )\n",
    "    \n",
    "    \n",
    "elif baseline == 'FEDformer':\n",
    "    fEDformer = FEDformer( \n",
    "        n_steps = train_data.shape[1], n_features = train_data.shape[2],\n",
    "        n_layers= 4, n_heads = 4,\n",
    "        d_model = 256, d_ffn = 256,\n",
    "        moving_avg_window_size = 24 ,\n",
    "        dropout = 0.1,\n",
    "        batch_size=200, epochs=200,  )\n",
    "    \n",
    "    fEDformer.fit( train_set = train_set  )\n",
    "    \n",
    "    \n",
    "elif baseline == 'DLinear':\n",
    "    dLinear = DLinear( \n",
    "        n_steps = train_data.shape[1], n_features = train_data.shape[2],\n",
    "        moving_avg_window_size= 5,\n",
    "        d_model=512,\n",
    "        ORT_weight=1, MIT_weight=1,\n",
    "        batch_size=200, epochs=300,  )\n",
    "    \n",
    "    dLinear.fit( train_set = train_set  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93a17078",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data * S\n",
    "train_data[train_data == 0] = np.nan\n",
    "\n",
    "\n",
    "'选择不同的测试集'\n",
    "test_way = 1\n",
    "# test_way = 2\n",
    "# test_way = 3\n",
    "\n",
    "\n",
    "'选择测试集方法1 常规方法 500个样本'\n",
    "'train_data和Xnan一样 '\n",
    "if test_way == 1:\n",
    "    test_set =  {\"X\": train_data[0:500] }\n",
    "#     test_set =  {\"X\":  Xnan[0:500]}\n",
    "\n",
    "\n",
    "# 选择测试集方法2 常规方法 全部样本\n",
    "elif test_way == 2:\n",
    "    test_set =  {\"X\": train_data }\n",
    "    # test_set =  {\"X\":  Xnan}\n",
    "\n",
    "    \n",
    "# 选择测试集方法3 \n",
    "# '取前500个样本  为了和dps-not方法的样本顺序一样 需要进行一次shuffle 然后取出来 再测试'\n",
    "elif test_way == 3:   \n",
    "\n",
    "    class MyDataset(Dataset):\n",
    "        def __init__(self, X_data, Y_data):\n",
    "            self.X_data = X_data\n",
    "            self.Y_data = Y_data\n",
    "        def __len__(self):\n",
    "            return len(self.X_data)\n",
    "        def __getitem__(self, idx):\n",
    "            x = torch.tensor(self.X_data[idx], dtype=torch.float32)    # 可以用dtype修改数据类型\n",
    "            y = torch.tensor(self.Y_data[idx])                         # 掩码修改数据类似的话会变成01\n",
    "            return x, y   \n",
    "\n",
    "\n",
    "    dataset_with_mask = MyDataset( Xnan[0:500], S[0:500] ) \n",
    "    dataloader_with_mask = DataLoader(dataset_with_mask, batch_size=500, shuffle=True) \n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    Xnan_shuffle = next(iter(dataloader_with_mask))[0]\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    S_shuffle = next(iter(dataloader_with_mask))[1].numpy()\n",
    "    print( Xnan_shuffle[0][0]  )\n",
    "    print( S_shuffle[0][0]  )\n",
    "\n",
    "\n",
    "\n",
    "    test_set =  {\"X\":  Xnan_shuffle}\n",
    "\n",
    "    # 原数据集也要shuffle一下\n",
    "    torch.manual_seed(0)\n",
    "    dataset_with_mask = MyDataset( load_data[0:500], S[0:500] ) \n",
    "    dataloader_with_mask = DataLoader(dataset_with_mask, batch_size=500, shuffle=True) \n",
    "    test_load_data = next(iter(dataloader_with_mask))[0].numpy()\n",
    "    print( test_load_data[0][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3be11f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 30, 6)\n"
     ]
    }
   ],
   "source": [
    "if baseline == 'Pyraformer':\n",
    "    result_base = pyraformer.predict(test_set)\n",
    "    \n",
    "elif baseline == 'SAITS':\n",
    "    result_base = saits.predict(test_set)\n",
    "    \n",
    "elif baseline == 'ImputeFormer':\n",
    "    result_base = imputeFormer.predict(test_set)\n",
    "    \n",
    "elif baseline == 'Informer':\n",
    "    result_base = informer.predict(test_set)\n",
    "    \n",
    "elif baseline == 'ETSformer':\n",
    "    result_base = eTSformer.predict(test_set)\n",
    "    \n",
    "elif baseline == 'TimesNet':\n",
    "    result_base = timesNet.predict(test_set)\n",
    "    \n",
    "elif baseline == 'TimeMixer':\n",
    "    result_base = timeMixer.predict(test_set)\n",
    "    \n",
    "elif baseline == 'DLinear':\n",
    "    result_base = dLinear.predict(test_set)  \n",
    "\n",
    "elif baseline == 'CSDI':\n",
    "    result_base = csdi.predict(test_set, n_sampling_times=2)  \n",
    "    \n",
    "    \n",
    "    \n",
    "if baseline == 'CSDI':\n",
    "    result_base = result_base[\"imputation\"]\n",
    "    result_base = result_base.mean(axis=1)\n",
    "else:   \n",
    "    result_base = result_base[\"imputation\"]\n",
    "    \n",
    "print(result_base.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84062776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe3373cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0243 0.0174\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import calc_mse,calc_rmse,calc_mae\n",
    "\n",
    "'方法1测试集的rmse 500个样本'\n",
    "if test_way == 1:\n",
    "    rmse = calc_rmse(result_base, load_data[0:500], 1- S[0:500])\n",
    "    mae = calc_mae(result_base, load_data[0:500], 1- S[0:500]) \n",
    "    print(round(rmse,4), round(mae,4) )\n",
    "\n",
    "\n",
    "# '方法2测试集的rmse 全部样本'\n",
    "elif test_way == 2:\n",
    "    rmse = calc_rmse(result_base, load_data, 1- S)\n",
    "    mae = calc_mae(result_base, load_data, 1- S) \n",
    "    print(round(rmse,4), round(mae,4) )\n",
    "\n",
    "\n",
    "# '方法3的测试集的rmse'\n",
    "elif test_way == 3:\n",
    "    rmse = calc_rmse( result_base, test_load_data, 1-S_shuffle )\n",
    "    mae = calc_mae( result_base, test_load_data, 1-S_shuffle) \n",
    "    print(round(rmse,4), round(mae,4) )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3520c272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e2b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff68b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55456b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588344c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b34f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c37827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e77b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d409a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abdb0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52d5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c9a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540f5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c88e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
